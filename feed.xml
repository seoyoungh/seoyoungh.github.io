<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://seoyoungh.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://seoyoungh.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-02-23T05:06:07+00:00</updated><id>https://seoyoungh.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Euclidean Distance, Manhattan Distance 등 여러가지 distance metrics 알아보기</title><link href="https://seoyoungh.github.io/deep-learning/distance-metrics/" rel="alternate" type="text/html" title="Euclidean Distance, Manhattan Distance 등 여러가지 distance metrics 알아보기"/><published>2021-05-07T00:00:00+00:00</published><updated>2021-05-07T00:00:00+00:00</updated><id>https://seoyoungh.github.io/deep-learning/distance-metrics</id><content type="html" xml:base="https://seoyoungh.github.io/deep-learning/distance-metrics/"><![CDATA[<p>Recommender systems 원서를 읽던 중에 Euclidean Distance와 Manhattan Distance가 등장했다. 두 개념을 비교하고, 왜 특정 상황에서 Euclidean Distance보다 Manhattan Distacne를 사용하는 것이 나은지 알아보자. GAN에서 사용하는 거리 개념도 살짝 맛보자.</p> <h3 id="distance를-구하는-이유">Distance를 구하는 이유?</h3> <ul> <li>distances는 결국 일종의 similarity 개념이다. 거리가 가까우면 유사한 것, 멀면 유사하지 않은 것이다.</li> <li>추천 시스템은 물론, NLP 도메인에서 문서의 유사도를 구할때 등 다양한 인공지능 분야에서 널리 사용된다.</li> <li>데이터 특징에 맞게 적절한 metric을 선택해야 한다.</li> </ul> <p><br/></p> <h3 id="euclidean-distance">Euclidean distance</h3> <p>2차원이라고 가정하면 유클리드 거리를 구하는 식은 아래와 같다.</p> <p>$d=\sqrt{(a_1-b_1)^2+(a_2-b_2)^2}$</p> <p>기하학적으로 생각하면 L2 Norm, 점 a와 b의 최단 거리를 구하는 것이다. 대표적인 common distance metric이다.</p> <p><br/></p> <h3 id="manhattan-distance">Manhattan distance</h3> <p><img src="/assets/images/mht.png" alt="mht" width="30%" height="30%"/></p> <p>유클리드 거리와 달리, 차원의 차를 제곱하지 않고 절대값의 합으로 나타낸다. ‘Manhattan’ 이라는 이름의 유래처럼, 몇 블록 이동했는지를 계산하는 metric이라고 직관적으로 이해하자. N1 Norm이다.</p> <p><br/></p> <h3 id="유클리드-거리보다-맨하탄맨하튼-거리가-선호되는-경우">유클리드 거리보다 맨하탄/맨하튼 거리가 선호되는 경우</h3> <h4 id="multidimensional-and-sparse-data">Multidimensional and sparse data</h4> <p>Recommender Systems 책 peers clustering 46페이지에 잠깐 언급된 부분이다. 모든 유저가 많은 rating을 남기는 것이 아니기 때문에 유저마다 몇 개에 대해 평가를 했는지 그 수가 다를 수 있다. 이와 같은 이유로 rating matrix를 그렸을 때 유저들의 dimensions의 차이가 크다면, 유클리드보다 맨하탄을 사용하는 게 낫다라고 한다. 또는 normalized value를 사용하는 것이 better.</p> <p>The use of Manhattan distance depends a lot on the kind of co-ordinate system that your dataset is using. <strong>While Euclidean distance gives the shortest or minimum distance between two points, Manhattan has specific implementations.</strong></p> <p>For example, if we were to use a Chess dataset, the use of Manhattan distance is more appropriate than Euclidean distance. Another use would be when are interested in knowing the distance between houses which are few blocks apart.</p> <p>Also, you might want to consider Manhattan distance if the input variables are not similar in type (such as age, gender, height, etc.). Due to <strong>the curse of dimensionality</strong>, we know that Euclidean distance becomes a poor choice as the number of dimensions increases.</p> <p>So in a nutshell: Manhattan distance generally works only if the points are arranged in the form of a grid and the problem which we are working on gives more priority to the distance between the points only along with the grids, but not the geometric distance.</p> <p><br/></p> <h4 id="high-dimensional-data">High-dimensional data</h4> <p>Most of the volume of a high-dimensional orange is in the skin, not the pulp. If a constant number of examples is distributed uniformly in a high-dimensional hypercube, <strong>beyond some dimensionality most examples are closer to a face of the hypercube than to their nearest neighbor.</strong></p> <p>And if we approximate a hypersphere by inscribing it in a hypercube, in high dimensions almost all the volume of the hypercube is outside the hypersphere. This is bad news for machine learning, where shapes of one type are often approximated by shapes of another.</p> <p>In high dimensions, a curious phenomenon arises: <strong>the ratio between the nearest and farthest points approaches 1</strong>, i.e. the points essentially become uniformly distant from each other. <strong>This phenomenon can be observed for wide variety of distance metrics, but it is more pronounced for the Euclidean metric than, say, Manhattan distance metric.</strong></p> <p>The premise of nearest neighbor search is that “closer” points are more relevant than “farther” points, but if all points are essentially uniformly distant from each other, the distinction is meaningless.</p> <p><br/></p> <h4 id="summary">Summary</h4> <p>유클리드 거리는 고차원에서는 도움이 되지 않는다. 차원이 커지면서 대부분 유사한 거리를 갖게 되기 때문이다. 하나의 점을 중심으로 원을 그리고, 그 위에 다른 점들이 있다고 생각하면 거리는 같다.</p> <p>데이터의 차원이 다른 경우에도, shortest distance를 구하는 Euclidean distacne 보다는, 절대적인 L1 norm을 구하는 metric인 Manhattan distance가 낫다.</p> <p>이런 점을 보완하려고 0~1 사이의 값을 갖도록 fraction을 쓰는 $L_f$ norm 같은 친구들도 있는듯.</p> <p>직관적으로 생각해도 제곱을 해주는 유클리드 거리보다는 절댓값을 구하는 맨하튼이 해당 상황들에선 낫지 않을까?</p> <p>The higher the norm index, the more it focuses on large values and neglects small ones. This is why the RMSE is more sensitive to outliers than the MAE.</p> <p>norm의 지수가 클 수록 큰 값의 원소에 치우치고 작은 값은 무시되는 경향이 있다. high dimension에서 RMSE가 MAE보다 outlier에 취약한 이유도 같은 맥락.</p> <p><br/></p> <h4 id="관련-stackexchange-질문들">관련 stackexchange 질문들</h4> <p><a href="https://datascience.stackexchange.com/questions/20075/when-would-one-use-manhattan-distance-as-opposed-to-euclidean-distance">When would one use Manhattan distance as opposed to Euclidean distance?</a></p> <p><a href="https://stats.stackexchange.com/questions/29627/euclidean-distance-is-usually-not-good-for-sparse-data-and-more-general-case">Euclidean distance is usually not good for sparse data (and more general case)?</a></p> <p><a href="https://stats.stackexchange.com/questions/99171/why-is-euclidean-distance-not-a-good-metric-in-high-dimensions/99191#99191">Why is Euclidean distance not a good metric in high dimensions?</a></p> <p><br/></p> <h3 id="gan에서-distance를-사용하는-방법">GAN에서 distance를 사용하는 방법</h3> <p>GAN은 내 분야는 아니지만 어쩌다 읽은 논문에서 해당 주제를 다뤄서, 잠깐 정리해둔 것을 아래에 첨부한다. 조금 불친절하게 정리되어 있지만, 결국 GAN에서도 생성 모델 학습 과정에서 생성 모델이 만든 분포와 실제 타겟으로 하는 분포의 거리를 구한다는 것.</p> <p><br/></p> <h4 id="generative-model">Generative Model</h4> <ul> <li>학습 data의 분포를 학습해 해당 분포를 따르는 유사한 data를 생성하는 Model</li> <li>P_g: 생성 모델이 만들어낸 분포</li> <li>P_x: target으로 하는 분포</li> <li>P_g -&gt; P_x 로 만들 때 optimal transport는 <strong>발생한 평균 cost</strong> (P_g를 일부 변형시켜 P_g’를 만듦)의 하한</li> </ul> <p><br/></p> <h4 id="optimal-transport-mapping">Optimal transport mapping</h4> <ul> <li>두 공간을 어떻게 연결시켰을 때 가장 최적인 경로를 찾을 수 있을지</li> </ul> <p><br/></p> <h4 id="wasserstein-distance">Wasserstein distance</h4> <ul> <li><a href="https://en.wikipedia.org/wiki/Wasserstein_metric">reference: Wasserstein Metric Wiki</a></li> <li><a href="https://www.slideshare.net/ssuser7e10e4/wasserstein-gan-i">reference: Wasserstein GAN 수학 이해하기 I</a></li> <li>거리 척도 중 하나.</li> <li>Wasserstein GAN에서 discriminator가 학습 도중 잘 죽는 현상을 방지하기 위해 제안</li> </ul>]]></content><author><name></name></author><category term="deep-learning"/><category term="machine-learning"/><category term="deep-learning"/><category term="recommender-systems"/><summary type="html"><![CDATA[Recommender systems 원서를 읽던 중에 Euclidean Distance와 Manhattan Distance가 등장했다. 두 개념을 비교하고, 왜 특정 상황에서 Euclidean Distance보다 Manhattan Distacne를 사용하는 것이 나은지 알아보자. GAN에서 사용하는 거리 개념도 살짝 맛보자.]]></summary></entry><entry><title type="html">피어슨 상관계수와 adjusted 코사인 유사도의 차이</title><link href="https://seoyoungh.github.io/machine-learning/recommender-systems/cosine-pearson/" rel="alternate" type="text/html" title="피어슨 상관계수와 adjusted 코사인 유사도의 차이"/><published>2021-04-21T00:00:00+00:00</published><updated>2021-04-21T00:00:00+00:00</updated><id>https://seoyoungh.github.io/machine-learning/recommender-systems/cosine-pearson</id><content type="html" xml:base="https://seoyoungh.github.io/machine-learning/recommender-systems/cosine-pearson/"><![CDATA[<p>Recommender systems 원서를 읽던 중에 두 식이 매우 유사해 차이를 정확하게 정리하려 한다.</p> <p>우선 코사인 유사도, 피어슨 기반 유사도, adjusted 코사인 유사도의 식을 보겠다.</p> <p>세 지표 모두 user-user간, 또는 item-item간 얼마나 유사한지를 나타내는 지표이다.</p> <p><strong>아래 식은 item-based collaborative filtering 과정에 대해 쓰여진 식입니다. user-based CF라면 그에 맞게 바꿔 생각해주시면 됩니다.</strong></p> <p><a href="https://www.cs.carleton.edu/cs_comps/0607/recommend/recommender/itembased.html">이미지 및 설명 출처</a></p> <p><br/></p> <h3 id="cosine-similarity">Cosine similarity</h3> <p>Also known as vector-based similarity, this formulation views two items and their ratings as vectors, and defines the similarity between them as the angle between these vectors:</p> <p>두 벡터 간의 유사한 정도를 코사인 값으로 나타낸 것이다. -1부터 1까지의 값을 가지며, -1은 서로 전혀 다르고 1은 완전히 같은 경우를 의미한다.</p> <p><img src="/assets/images/itembased-cosine.png" alt="itembased-cosine"/></p> <p><br/></p> <h3 id="pearson-based-similarity">Pearson based similarity</h3> <p>This similarity measure is based on how much the ratings by common users for a pair of items deviate from average ratings for those items:</p> <p>유저의 rating 기준은 유저마다 다를 수 있다. 1부터 5까지의 rating 범위를 갖는다고 하자. 어떤 positive 유저는 맘에 들지 않은 아이템에게도 3 이상의 값을 줄 수 있다. 반면 negative 유저는 조금이라도 취향에 맞지 않으면 1의 rating을 부여할 수도 있다. 따라서 이러한 different rating scales를 보정하기 위해 <code class="language-plaintext highlighter-rouge">mean-centered</code> rating을 사용한다.</p> <p><img src="/assets/images/itembased-pearson.png" alt="itembased-pearson"/></p> <p>위 식과 같이 raw cosine에서 해당 item에 부여된 모든 rating의 평균값을 빼주어 보정한다.</p> <p><br/></p> <h3 id="adjusted-cosine-similarity">Adjusted Cosine similarity</h3> <p>This similarity measurement is a modified form of vector-based similarity where we take into the fact that different users have different ratings schemes; in other words, some users might rate items highly in general, and others might give items lower ratings as a preference. To remove this drawback from vector-based similarity, we subtract average ratings for each user from each user’s rating for the pair of items in question:</p> <p><img src="/assets/images/itembased-adjusted-cosine.png" alt="itembased-adjusted-cosine"/></p> <p>mean-centered 보정을 해주는 것은 피어슨 상관계수 접근과 같지만, 빼주는 값이 <code class="language-plaintext highlighter-rouge">mu_u</code>로 같은 값이다. 피어슨 상관계수 식에서는 <code class="language-plaintext highlighter-rouge">mu_i</code>, <code class="language-plaintext highlighter-rouge">mu_j</code>로 각각 빼주는 값이 달랐는데 말이다. 이는 adjust cosine similarity를 구할 때는 item에 부여된 rating의 평균이 아니라, 해당 유저가 모든 아이템에 부여한 rating의 평균값을 빼주는 것이다.</p> <p><br/></p> <h3 id="summary">Summary</h3> <p>두 유사도는 유사하지만, mean을 어떻게 구해주냐에 따라 전혀 다른 값을 가진다. 두 유사도가 같다는 오해를 하지 않길 바란다. 참고로, 원서에서는 item-based CF에서는 pearson-based보다 adjusted cosine이 더 성능이 높다고 했다. user-based에서는 pearson을 언급한 걸 보니 pearson이 더 효과적인 지표 아닐까..라고 추측한다. 언제 어떤 지표를 사용하는 것이 더 유리한지는 더 공부해보기.</p> <p><br/></p> <h4 id="user-based-cf에서는">User-based CF에서는?</h4> <h5 id="pearson-based-similarity-1">Pearson based similarity</h5> <ul> <li>user-based에서는 item_i와 item_j간의 유사도가 아닌, user_i, user_j의 유사도를 구하기 때문에 유저 i, j 각각의 모든 rating의 평균을 빼주면 된다.</li> <li>Strictly, 유저 i와 j가 공통적으로 rating을 남긴 item들에 대한 평균만 구해야한다. 하지만, 현실의 문제는 sparsity가 높기 때문에 그냥 전체 item의 평균을 사용한다고 한다.</li> </ul> <p><br/></p> <h5 id="adjusted-cosine-similarity-1">Adjusted Cosine similarity</h5> <ul> <li>user_i, user_j가 공통적으로 rating을 남긴 item k의 rating 평균을 적용하면 되겠다.</li> </ul>]]></content><author><name></name></author><category term="machine-learning"/><category term="recommender-systems"/><category term="machine-learning"/><category term="recommender-systems"/><summary type="html"><![CDATA[Recommender systems 원서를 읽던 중에 두 식이 매우 유사해 차이를 정확하게 정리하려 한다.]]></summary></entry><entry><title type="html">베이즈 추정법과 최대가능도추정법</title><link href="https://seoyoungh.github.io/today-i-learned/bayes-mle/" rel="alternate" type="text/html" title="베이즈 추정법과 최대가능도추정법"/><published>2021-04-14T00:00:00+00:00</published><updated>2021-04-14T00:00:00+00:00</updated><id>https://seoyoungh.github.io/today-i-learned/bayes-mle</id><content type="html" xml:base="https://seoyoungh.github.io/today-i-learned/bayes-mle/"><![CDATA[<p>모수 추정의 끝판왕 친구들! 두 개념은 자주 쓰이지만, 처음 접하면 꽤 헷갈리는 개념이므로 확실하게 짚고 넘어가자! <code class="language-plaintext highlighter-rouge">인공지능을 위한 수학</code> 책에서 내용의 일부를 발췌했다. (책 추천합니다. 하나 책꽂이에 있으면 든든해요.)</p> <h3 id="최대가능도추정">최대가능도추정</h3> <p><img src="/assets/images/mle.png" alt="mle" width="30%" height="30%"/></p> <ul> <li>Maximum Likelihood Estimation, MLE</li> <li>최대가능도를 추정한다는 것은 파라미터 $\theta$에 대한 가능도함수 L($\theta$)를 최대화하는 모수 $\theta$ 값을 구하는 것이다.</li> <li>이때, 최댓값을 가지는 지점은 1계 미분을 했을 때 $dL(\theta)/d\theta = 0$이 되는 지점이며, 이때의 $\theta$를 구하면 된다.</li> </ul> <p><br/></p> <h4 id="example">Example</h4> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>예를 들어, 주사위를 던져 숫자 1이 나올 확률은 1/6이다.
하지만 단언할 수는 없기 때문에 확률을 미지의 수로 두고 실험을 해보자.
만약 100번을 던졌을 때, 숫자 1이 나온게 20번이라고 하자.
</code></pre></div></div> <p>이때 1이 나올 확률을 $\theta$라고 두고, 위와 같은 결과가 나올 확률을 $L(\theta)$라고 하자. 그러면 아래와 같은 식이 나온다.</p> <p><img src="/assets/images/mle_eg.png" alt="mle_eg" width="30%" height="30%"/></p> <p>이 식을 구했으니, 이 식을 미분해 0이 나오는 $\theta$를 구하면 된다. 하지만, 이 식의 차수는 매우 높기 때문에 미분을 하기엔 복잡하다. 따라서, 이 가능도함수에 자연로그를 붙여주어 <strong>로그가능도함수</strong>를 만들어주면 편하다.</p> <p><br/></p> <h4 id="확률과-가능도의-차이">확률과 가능도의 차이</h4> <ul> <li>확률: 사건이 어떤 빈도로 일어날 것인지 나타낸 것</li> <li>가능도: 관측된 값들을 생성할 가능성이 가장 높은 실제 모수를 추정. 이때의 가능성을 수치로 표현한 것이 가능도이다.</li> </ul> <p><br/></p> <h4 id="로그가능도함수">로그가능도함수</h4> <p><img src="/assets/images/log_mle.png" alt="log_mle" width="20%" height="20%"/></p> <p>로그를 사용하면, 곱셈을 덧셈으로 바꿀 수 있어 고차 방정식을 1차 방정식 모양으로 바꿀 수 있다.</p> <p><br/></p> <h4 id="summary">Summary</h4> <p>주사위 예제에서 로그가능도함수를 미분한 후 0이 되는 지점의 $\theta$를 찾으면, 0.2가 된다.</p> <p>이를 통해 우리는 “어떤 주사위를 던졌을 때, 숫자 1이 나올 확률로 <strong>가장 그럴듯한 것은</strong> 0.2이다.”라는 결론을 얻을 수 있다.</p> <p>파라미터가 여러 개인 경우는 각각의 파라미터에 대해 편미분을 하면 된다.</p> <p><br/></p> <h3 id="베이즈-추정법">베이즈 추정법</h3> <p>최대가능도 추정법과 가장 다른 점은, 모수 $\theta$를 미지의 수로 보지 않고 확률변수로 본다는 점이다. 확률변수는 확률밀도함수를 갖고, 어떤 값이 가능성이 높고 어떤 값이 낮은지를 살펴본다는 것이다.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>관찰을 통해 얻은 데이터는 사전분포에 따라 얻어진 결과이므로,
그에 대한 조건부확률을 구하면 된다.
</code></pre></div></div> <p>라는 접근 방식으로 <strong>사후확률(조건부확률)</strong> 을 구한다. 결국 관측된 데이터를 기반으로 모수의 조건부 확률분포를 계산하는 작업이다.</p> <p>베이즈 정리, 사후확률을 추정하는 식은 아래와 같다. B는 관측값, A는 실제값이라고 직관적으로 생각해보자.</p> <p><img src="/assets/images/bayes.png" alt="bayes" width="30%" height="30%"/></p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>P(A|B): 사후확률
- 사건 B를 관측한 후에, 그 원인이 되는 사건 A의 확률을 따졌다는 의미
P(B|A): 우도
P(A): 사전확률
- 사전에 미래에 어떤 사건이 일어날 확률을 측정한 것
</code></pre></div></div> <p><br/></p> <h4 id="베이즈-추정법을-쓰는-이유">베이즈 추정법을 쓰는 이유</h4> <p>베이즈 추정법을 사용하는 이유는 <strong>추정된 모숫값 숫자 하나만으로는 추정의 신뢰도와 신뢰구간을 구할 수 없기 때문이다.</strong></p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>예를 들어 인터넷 쇼핑몰에 있는 두 개의 경쟁상품에 사용자 의견이 다음과 같이 붙어 있다고 하자.

상품 A: 전체 평가의견 3개, 좋아요 2개, 싫어요 1개
상품 B: 전체 평가의견 100개, 좋아요 60개, 싫어요 40개
</code></pre></div></div> <p>내가 이 상품을 사용했을 때 평가의견이 ‘좋아요’가 나올지 ‘싫어요’가 나올지는 베르누이분포 확률변수로 모형화할 수 있다. 최대가능도 추정법에 따르면 상품 A와 상품 B에서 ‘좋아요’가 나온 비율을 사용하여 베르누이 모수를 구하면 다음과 같이 상품 A의 모수가 높다.</p> <p>상품 A의 모수: $\frac{2}{3} = 0.67$ 상품 B의 모수: $\frac{60}{100} = 0.6$</p> <p>상품 B의 평가의견은 100개고 상품 A의 평가의견은 3개밖에 되지 않는데 상품 A의 모수가 더 높다고 더 높은 상품이라고 확신할 수 있는가? <strong>베이즈 추정법에서는 단순히 모수의 값을 하나의 숫자로 구하는 것이 아니므로 이러한 잘못된 결론을 내리지 않도록 도와준다.</strong></p> <p><a href="https://brunch.co.kr/@chris-song/59">베이즈 정리를 재밌게 정리한 블로그. 코난..🐶</a> <a href="https://sumniya.tistory.com/29">기존 빈도론과 베이지안이 어떻게 다른지 설명한 블로그</a></p> <p><br/></p> <h3 id="comparison">Comparison</h3> <p>인공지능을 위한 수학 책에서는 둘을 다음과 같이 비교하고 있다. 둘 중 어느 것을 사용하더라도, 이는 모두 ‘추정’에 불과하므로 한계를 명확히 인지하고 데이터를 다뤄야 한다.</p> <p><br/></p> <h4 id="수학적으로-정확한-최대가능도추정법">수학적으로 정확한 최대가능도추정법</h4> <ul> <li>진정한 확률 모델은 존재하며, 관찰된 데이터는 그러한 모델을 충실히 따르고 있다. 따라서, 시행을 반복하여 관찰하면 진정한 확률 모델이 보이기 시작할 것이다.</li> <li>하지만, 실험을 무한하게 할 수는 없으니 우리는 가장 그럴듯한 확률, 관찰되는 데이터를 믿을 수 밖에 없다.</li> <li>따라서 관찰 결과가 편향적이라면 치명적인 추정법이다. 시행 횟수가 충분해야 신뢰할 수 있다.</li> </ul> <p><br/></p> <h4 id="실용적이지만-의심스러운-베이즈추정법">실용적이지만 의심스러운 베이즈추정법</h4> <ul> <li>지금까지의 관찰 결과를 근거로 사전분포를 가정한다. 이 사전분포에 따라 관찰 데이터가 얻어졌으므로, 그에 대한 조건부 확률을 구하면 된다고 한다.</li> </ul>]]></content><author><name></name></author><category term="today-i-learned"/><category term="statistics"/><category term="mathematics"/><category term="data-science"/><summary type="html"><![CDATA[모수 추정의 끝판왕 친구들! 두 개념은 자주 쓰이지만, 처음 접하면 꽤 헷갈리는 개념이므로 확실하게 짚고 넘어가자! 인공지능을 위한 수학 책에서 내용의 일부를 발췌했다. (책 추천합니다. 하나 책꽂이에 있으면 든든해요.)]]></summary></entry><entry><title type="html">Introduction of Recommender systems</title><link href="https://seoyoungh.github.io/machine-learning/recommender-systems/recommender-systems-2/" rel="alternate" type="text/html" title="Introduction of Recommender systems"/><published>2021-04-14T00:00:00+00:00</published><updated>2021-04-14T00:00:00+00:00</updated><id>https://seoyoungh.github.io/machine-learning/recommender-systems/recommender-systems-2</id><content type="html" xml:base="https://seoyoungh.github.io/machine-learning/recommender-systems/recommender-systems-2/"><![CDATA[<p>전반적인 추천 시스템에 대해 알아보자! Recommender systems 원서에서 발췌했습니다.</p> <h2 id="recommender-systems">Recommender Systems</h2> <h3 id="11-introduction">1.1 Introduction</h3> <ul> <li>Web의 등장으로 유저가 five-star ratings, like or dislike 등으로 feedback을 쉽게 제공할 수 있게 됨</li> <li>Recommendation analysis is often based on <strong>the previous interaction between users and items</strong></li> <li><strong>past interests &amp; proclivities</strong> -&gt; good indicators of future choices</li> <li>knowledge-based recommender system의 경우, past history보다는 user-specified requirements에 따라 추천</li> <li>the basic principle: significant dependencies exist between user- and item-centric activity</li> <li>various categories of items may show significant correlations</li> </ul> <p><br/></p> <h4 id="추천-시스템-종류">추천 시스템 종류</h4> <ul> <li>neighborhood models <ul> <li><strong>collaborative filtering</strong> <ul> <li>input: User ratings + community ratings</li> <li>the use of ratings from multiple users in a collaborative way to predict missing ratings</li> <li>work with data which is the information about the users and items such as textual profiles or relevant keywords</li> </ul> </li> </ul> </li> <li><strong>content-based</strong> recommender systems <ul> <li>input: User ratings + item attributes</li> <li>user interests can be modeled on the basis of attributes of the items they’ve rated or accessed in the past</li> <li>work with data which is the user-item interactions, such as ratings or buying behavior</li> </ul> </li> <li><strong>knowledge-based</strong> recommender systems <ul> <li>input: User specification + item attributes + domain knowledge</li> <li>specify their interests and the user specification is combined with domain knowledge</li> </ul> </li> </ul> <p><br/></p> <h3 id="12-goals-of-recommender-systems">1.2 Goals of Recommender Systems</h3> <h4 id="two-primary-models">Two primary models</h4> <h5 id="1-prediction-version-of-problem">1. Prediction version of problem</h5> <ul> <li>to predict the rating value for a user-item combination</li> <li>matrix completion problem</li> <li>incompletely specified matrix of values, and the remaining values are predicted by the learning algorithm</li> </ul> <p><br/></p> <h5 id="2-ranking-version-if-problem">2. Ranking version if problem</h5> <ul> <li>top-k recommendation problem</li> <li>ranking formulation of the recommendation problem</li> <li>not necessary to predict the ratings of users for specific items</li> <li>just recommend the top-k items for particular user, or the top-k users to target for a particular item</li> <li>recommend the top-k items -&gt; more popular</li> <li>the absolute values of the predicted ratings are not important</li> </ul> <p><br/></p> <h4 id="the-primary-goals">The primary goals</h4> <ul> <li><strong>increasing product sales and profits</strong></li> </ul> <p><br/></p> <h4 id="operational-and-technical-goals">Operational and technical goals</h4> <h5 id="1-relevance">1. Relevance</h5> <ul> <li>the most obvious operational goal</li> <li>to recommend items that are relevant to the user</li> </ul> <p><br/></p> <h5 id="2-novelty">2. Novelty</h5> <ul> <li>유저가 과거에 보지 못한 아이템을 추천하면 good</li> <li>단순히 popular한 아이템 추천을 반복하는 것은 sales diversity를 떨어뜨림</li> </ul> <p><br/></p> <h5 id="3-serendipity">3. Serendipity</h5> <ul> <li>유저가 평소에 선호하던 것과는 다르지만, 유저를 정말 놀래킬만한 것</li> <li>truly surprising to the user, rather than simply something they did not know about before</li> <li>increasing sales diversity or beginning a new trend of interest</li> <li>longer term and strategic benefits</li> </ul> <p><br/></p> <h5 id="4-increasing-recommendation-diversity">4. Increasing recommendation diversity</h5> <ul> <li>비슷한 추천만 하면 안되고 다양한 type의 아이템을 추천 리스트에 포함시켜야 함</li> </ul> <p><br/></p> <h4 id="other-goals">Other goals</h4> <ul> <li>help improve overall user satisfaction</li> <li><strong>explanation for why a particular item is recommended</strong> <ul> <li>e.g.) previously watched movies</li> </ul> </li> </ul> <p><br/></p> <h4 id="example-of-products-recommended">Example of products recommended</h4> <ul> <li>GroupLens: News <ul> <li>pioneering recommender system</li> <li>BookLens, MovieLens</li> </ul> </li> <li>Amazon.com: Books and other products <ul> <li>implicit ratings: purchase or browsing behavior of a user</li> <li>providing recommendations both on the basis of explicit and implicit feedback</li> </ul> </li> <li>Netflix: Videos <ul> <li>providing explanations</li> </ul> </li> <li>Google News: News <ul> <li>implicit ratings</li> </ul> </li> <li>Facebook: Friends, Advertisements <ul> <li><strong>link prediction</strong></li> <li>based on structural relationships</li> </ul> </li> </ul> <p><br/></p> <h4 id="trends">Trends</h4> <ul> <li> <p>Traditional domain of recommender systems: traditional e-commerce applications for various products (books, movies, videos, other goods and services)</p> </li> <li> <p>최근에는 이러한 상품에 국한되지 않고 domain이 다양해짐</p> <ul> <li>Facebook <ul> <li>recommends friends, not products</li> </ul> </li> <li>Google Search <ul> <li>computational advertising</li> <li>검색 결과와 관련있는 상품을 광고</li> <li>추천과 같지는 않지만 closely related to recommender systems</li> </ul> </li> </ul> </li> </ul> <p><br/></p> <h3 id="13-basic-models-of-recommender-systems">1.3 Basic Models of Recommender Systems</h3> <h4 id="131-collaborative-filtering-models">1.3.1 Collaborative Filtering Models</h4> <ul> <li>문제점: the underlying rating matrices are sparse, most of the ratings are unspecified (unobserved)</li> <li>can be viewed as generalizations if classification and regression modeling</li> <li>there is no distinction between training and test rows in collaborative filtering because any row might contain missing values <ul> <li>training and test entries (O)</li> <li>training and test rows (X)</li> </ul> </li> </ul> <p><br/></p> <h5 id="1-memory-based-methods">1) Memory-based methods</h5> <ul> <li><strong>similarity-based models</strong></li> <li>Two types <ul> <li>User-based: 유저와 유사한 유저를 찾음</li> <li>Item-based: 유저가 선호하는 아이템과 유사한 아이템을 찾음</li> </ul> </li> <li>장점: simple to implement and the resulting recommendations are often easy to explain</li> <li>한계: do not work very well with sparse rating matrices</li> </ul> <p><br/></p> <h5 id="2-model-based-methods">2) Model-based methods</h5> <ul> <li>machine learning and data mining methods are used in the context of predictive models</li> </ul> <p><br/></p> <h4 id="132-content-based-recommender-systems">1.3.2 Content-based Recommender Systems</h4> <ul> <li>the descriptive attributes of items are used to make recommendations</li> <li><code class="language-plaintext highlighter-rouge">content</code> refers to <code class="language-plaintext highlighter-rouge">descriptions</code></li> <li>장점 <ul> <li>충분한 rating이 존재하지 않아도 추천 제공 가능</li> <li>provide obvious recommendations because of the use of keywords or content</li> </ul> </li> <li>약점 <ul> <li>they are not effective at providing recommendations for new users</li> <li>오버피팅 없이 robust한 예측을 하려면, 충분한 rating이 확보되어 있어야 함</li> </ul> </li> <li>new user가 가입할 때 preference keywords를 입력할 수도 있지만, 이는 knowledge-based에 가까운 추천 방식일 수 있음</li> </ul> <p><br/></p> <h4 id="133-knowledge-based-recommender-systems">1.3.3 Knowledge-based Recommender Systems</h4> <ul> <li>useful in the context of items are not purchased very often <ul> <li>real estate, automobiles, tourism requests, financial services, expensive luxury goods</li> <li><code class="language-plaintext highlighter-rouge">the cold-start problem</code>: 추천 제공을 위한 충분한 history data가 없음</li> <li>또는, 충분한 데이터가 있더라도 이를 통해 유저의 선호를 fully 파악하기 어려울 때 - 특정 브랜드, 측정 모델만을 선호하는 경우</li> </ul> </li> </ul> <p><br/></p> <h5 id="the-types-of-interface">The types of interface</h5> <ol> <li>Constraint-Based <ul> <li>users specify requirements or constraints</li> </ul> </li> <li>Case-based <ul> <li>specific cases are specified by the user as targets or anchor points</li> <li>similarity metrics are defined on the item attributes to retrieve similar items to these cases</li> <li>the similarity metrics often carefully defined in a domain-specific way</li> </ul> </li> </ol> <p><br/></p> <h5 id="the-interactivity-in-knowledge-based-recommender-systems">The interactivity in knowledge-based recommender systems</h5> <ol> <li>Conversational systems <ul> <li>유저의 preference는 반복적인 feedback loop를 통해 결정됨</li> </ul> </li> <li>Search-based systems <ul> <li>preset sequence of questions를 통해 유저의 preference를 결정</li> </ul> </li> <li>Navigation-based systems <ul> <li>현재 추천된 아이템에 대해 유저가 change requests를 specify함</li> </ul> </li> </ol> <p><br/></p> <h4 id="기타">기타</h4> <ul> <li>Utility-based Recommender Systems</li> <li>Demographic Recommender Systems</li> <li>Hybrid and Ensemble-Based Recommneder Systems</li> </ul> <p><br/></p> <h3 id="16-summary">1.6 Summary</h3> <h4 id="앞으로-다룰-것들">앞으로 다룰 것들</h4> <ul> <li>다양한 추천 시스템의 장단점</li> <li>different domain-specific scenarios, different types of input information, knowledge-bases</li> <li>advanced topics: attack models, group recommender systems, multi-criteria systems, active learning systems</li> </ul>]]></content><author><name></name></author><category term="machine-learning"/><category term="recommender-systems"/><category term="machine-learning"/><category term="recommender-systems"/><summary type="html"><![CDATA[전반적인 추천 시스템에 대해 알아보자! Recommender systems 원서에서 발췌했습니다.]]></summary></entry><entry><title type="html">추천 시스템의 종류</title><link href="https://seoyoungh.github.io/machine-learning/recommender-systems/recommender-systems-1/" rel="alternate" type="text/html" title="추천 시스템의 종류"/><published>2021-03-11T00:00:00+00:00</published><updated>2021-03-11T00:00:00+00:00</updated><id>https://seoyoungh.github.io/machine-learning/recommender-systems/recommender-systems-1</id><content type="html" xml:base="https://seoyoungh.github.io/machine-learning/recommender-systems/recommender-systems-1/"><![CDATA[<p>추천 시스템을 개괄적으로 다룬 논문을 짧게 요약한 것입니다.</p> <h3 id="the-survey-of-recommender-systems">The Survey of Recommender Systems</h3> <ul> <li>rely on the ratings structure</li> <li>the problem: estimating ratings of <strong>non-rated</strong> items</li> </ul> <p><br/></p> <h4 id="utility-function">Utility function</h4> <ul> <li>measures the usefulness of item <code class="language-plaintext highlighter-rouge">s</code> to user <code class="language-plaintext highlighter-rouge">c</code></li> <li>want to choose <code class="language-plaintext highlighter-rouge">s</code> to maximizes the users’s Utility</li> <li>utility is represented by a rating</li> </ul> <p><br/></p> <h5 id="문제점">문제점</h5> <ul> <li>전체 C*S space에서 정의되는 것이 아니라, only on subset of it! 유저들이 rated 했던 아이템에 대해서만 정의됨</li> <li><strong>Recommendation engine should be able to estimate the ratings of the nonrated item/user combination</strong></li> </ul> <p><br/></p> <h5 id="해결방법">해결방법</h5> <p>1) specifying heuristics that define the utility function 2) estimating the utility function that optimizes certain performance criterion</p> <ul> <li>machine learning, approximation theory, various heuristics …</li> </ul> <p><br/></p> <h4 id="추천시스템-종류">추천시스템 종류</h4> <ul> <li>Content-based: 과거에 선호했던 아이템과 유사한 아이템을 찾아 추천</li> <li>Collaborative filtering: 해당 유저와 가장 유사한 취향, 선호를 가진 사람들이 과거에 좋아했던 아이템을 추천</li> <li>Hybrid approaches: Content-based + Collaborative filtering</li> <li>cf) preference-based filtering: predicting the <code class="language-plaintext highlighter-rouge">relative</code> preferences of users</li> </ul> <p><br/></p> <h4 id="content-based">Content-based</h4> <ul> <li>usually focus on recommending items containing textual information</li> <li>the content is usually described with keywords, and calculate the importance of those keywords in the document</li> <li><strong>TF-IDF</strong>: 다른 문서에는 잘 안나오지만 그 문서에서 상대적으로 많이 나오는 term의 importance를 weight으로 나타냄</li> <li><strong>Cosine Similarity</strong>: 추천할 item vector와 user taste vector의 유사도를 계산 (heuristic method)</li> <li>measure the similarity between vectors of TF-IDF weights</li> <li><strong>Based on model</strong>: statistical learning, machine learning</li> </ul> <p><br/></p> <h5 id="문제점-1">문제점</h5> <p>1) Limited Content Analysis 2) Overspecialization 3) New User Problem</p> <p><br/></p> <h4 id="collaborative-filtering">Collaborative Filtering</h4> <ul> <li>predict the utility of items previously rated by other users</li> <li>tries to find the peers of user 1) memory-based</li> <li>essentially are heuristics</li> <li>the value of the unknown ratings -&gt; computed as an aggregation of previously rated items</li> <li>(average, weighted sum, adjusted weighted sum)</li> <li>the similarity between two users -&gt; based on their ratings of items that both users have rated</li> <li>(correlation, cosine-based)</li> <li>measures the similarity between vectors of the actual user-specified ratings</li> </ul> <p>2) model-based</p> <ul> <li>cluster models, Bayesian networks, a probabilistic relational model, linear regression</li> </ul> <p><br/></p> <h5 id="문제점-2">문제점</h5> <p>1) New User Problem 2) New Item Problem 3) Sparsity: user가 많아야 하고 user가 items를 많이 rated 했어야 함, 보완하기 위해 demographic 정보 사용 or dimensionality reduction technique</p> <p><br/></p> <h4 id="hybrid-methods">Hybrid Methods</h4> <ul> <li>다양한 방법으로 둘을 조합해 사용할 수 있음</li> </ul>]]></content><author><name></name></author><category term="machine-learning"/><category term="recommender-systems"/><category term="machine-learning"/><category term="recommender-systems"/><summary type="html"><![CDATA[추천 시스템을 개괄적으로 다룬 논문을 짧게 요약한 것입니다.]]></summary></entry><entry><title type="html">배치형, 스트리밍형의 데이터 플로우</title><link href="https://seoyoungh.github.io/data-science/distribute-system-5/" rel="alternate" type="text/html" title="배치형, 스트리밍형의 데이터 플로우"/><published>2020-08-13T00:00:00+00:00</published><updated>2020-08-13T00:00:00+00:00</updated><id>https://seoyoungh.github.io/data-science/distribute-system-5</id><content type="html" xml:base="https://seoyoungh.github.io/data-science/distribute-system-5/"><![CDATA[<p>아래 대부분의 내용은 도서 <strong>빅데이터를 지탱하는 기술</strong>에서 발췌했습니다.</p> <h4 id="map-and-reduce">Map and Reduce</h4> <ul> <li><code class="language-plaintext highlighter-rouge">Map</code>: 분할된 텍스트를 단어별로 수를 카운트</li> <li><code class="language-plaintext highlighter-rouge">Reduce</code>: 단어별 합계를 계산</li> <li>Map과 Reduce를 여러번 반복하며 결과를 얻음</li> </ul> <p><br/></p> <h4 id="데이터-플로우">데이터 플로우</h4> <ul> <li><code class="language-plaintext highlighter-rouge">데이터 플로우</code> <ul> <li>다단계의 데이터 처리를 그대로 분산 시스템 내부에서 실행하는 것</li> <li>MapReduce와 같은 외부 도구에 의존하는 <code class="language-plaintext highlighter-rouge">워크플로</code>와 구분</li> </ul> </li> <li>MapReduce를 <code class="language-plaintext highlighter-rouge">MillWheel</code>, <code class="language-plaintext highlighter-rouge">Tez</code>, <code class="language-plaintext highlighter-rouge">Spark</code> 등의 프레임워크가 대체하고 있음</li> </ul> <p><br/></p> <h5 id="dag">DAG</h5> <ul> <li><code class="language-plaintext highlighter-rouge">DAG</code> <ul> <li><strong>방향성 비순환 그래프 directed acyclic graph</strong></li> <li>새 프레임워크에 공통으로 들어가는 데이터 구조</li> <li>DAG의 성질 <ul> <li>방향성: 노드와 노드가 화살표로 연결된다.</li> <li>비순환: 화살표를 아무리 따라가도 동일 노드로는 되돌아오지 않는다.</li> </ul> </li> <li>MapReduce에서는 하나의 노드에서 처리가 끝나지 않으면 다음 처리로 진행할 수 없어 비효율적이었는데, 데이터 플로우에서는 DAG를 구성하는 각 노드가 모두 동시 병행으로 실행된다. 처리가 끝난 데이터는 네트워크를 거쳐 차례대로 전달되어 대기 시간을 없앤다.</li> <li><strong>lazy evaluation</strong> <ul> <li>프로그램의 각 행은 DAG의 데이터 구조를 조립하고 있을 뿐, 뭔가를 처리하지는 않음</li> <li>먼저 데이터 파이프라인 전체를 DAG로 조립하고 나서 실행해 옮김으로써 내부 스케줄러가 분산 시스템에 효과적인 실행 계획을 세워줌</li> </ul> </li> </ul> </li> </ul> <p><br/></p> <h5 id="단어를-세는-spark-프로그램-배치">단어를 세는 Spark 프로그램 (배치)</h5> <p>데이터 처리를 하는 파이썬 스크립트의 예시</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 파일로부터 데이터를 읽어들임
</span><span class="n">lines</span> <span class="o">=</span> <span class="n">sc</span><span class="p">.</span><span class="nf">textfile</span><span class="p">(</span><span class="sh">"</span><span class="s">sample.txt</span><span class="sh">"</span><span class="p">)</span>
<span class="c1"># 파일의 각 행을 단어로 분해
</span><span class="n">words</span> <span class="o">=</span> <span class="n">lines</span><span class="p">.</span><span class="nf">flatMap</span><span class="p">(</span><span class="k">lambda</span> <span class="n">line</span><span class="p">:</span> <span class="n">line</span><span class="p">.</span><span class="nf">split</span><span class="p">())</span>
<span class="c1"># 단어마다의 카운터를 파일에 출력
</span><span class="n">words</span><span class="p">.</span><span class="nf">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">word</span><span class="p">:</span> <span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span> \
     <span class="p">.</span><span class="nf">reduceByKey</span><span class="p">(</span><span class="k">lambda</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">:</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span><span class="p">)</span> \
     <span class="p">.</span><span class="nf">saveAsTextFile</span><span class="p">(</span><span class="sh">"</span><span class="s">word_counts</span><span class="sh">"</span><span class="p">)</span> <span class="c1"># 여기서 실행 개시
</span></code></pre></div></div> <p><br/></p> <h5 id="dag-1">DAG</h5> <ol> <li>textFile()</li> <li>flatMap()</li> <li>map()</li> <li>reduceByKey()</li> <li>saveAsTextFile()</li> </ol> <p><br/></p> <h4 id="데이터-플로우와-워크플로-조합하기">데이터 플로우와 워크플로 조합하기</h4> <ul> <li>태스크의 정기적인 실행, 실패한 태스크를 기록하여 복구하는 것은 데이터 플로우에서 할 수 없으므로 워크플로 관리가 필요함</li> <li>분산 시스템 안에서만 실행되는 데이터 처리라면 하나의 데이터 플로우로 기술할 수 있겠지만, 분산 시스템 외부와 데이터를 주고 받을 경우는 언제 어떤 오류가 생길지 모르므로 복구를 고려해 워크플로 안에서 실행하는 것이 바람직하다.</li> </ul> <p><br/></p> <h5 id="데이터-읽어-들이는--써서-내보내는-플로우">데이터 읽어 들이는 / 써서 내보내는 플로우</h5> <ul> <li>데이터 플로우 실행 과정에서 데이터 소스에 직접 액세스하면 성능 문제를 일으키기 쉽다.</li> <li>데이터 플로우로부터 <strong>읽어 들일 데이터</strong>는 성능적으로 안정된 분산 스토리지에 <strong>복사</strong>함으로써 안정적으로 사용한다.</li> <li>외부의 데이터 소스에서 데이터를 읽어 들일 때는 어떤 오류가 발생할 지 예측할 수 없으므로 워크플로, 벌크 형의 전송 도구로 태스크를 구현한다.</li> <li>분산 스토리지에 복사한 후로는 데이터 플로우의 전문 분야이다.</li> <li>외부 시스템에 <strong>써서 내보내는 경우</strong>에는, 분산 스토리지에 취급하기 쉬운 csv와 같은 형태로 변환해 써넣고, 워크플로 안에서 외부 시스템으로 데이터를 전송한다.</li> </ul> <p><br/></p> <h3 id="배치-처리">배치 처리</h3> <h4 id="스트리밍-형의-데이터-플로우">스트리밍 형의 데이터 플로우</h4> <ul> <li>데이터의 실시간 처리를 높이려면, 배치 처리와는 전혀 다른 데이터 파이프라인이 필요</li> <li>배치 처리를 중심으로 하는 데이터 파이프라인은 데이터가 분석할 수 있게 될 때까지 시간이 걸린다.</li> <li>예로, 열 지행 스토리지를 만들기 위해 데이터를 모아 변환하는 데 일정 시간이 필요하다.</li> <li><strong>이벤트 발생에서 몇 초 후에는 결과를 알 수 있는</strong> 실시간의 데이터 처리에서는 그런 과정을 생략한 별개의 계통으로 파이프라인을 만든다.</li> <li><code class="language-plaintext highlighter-rouge">스트림 처리</code> <ul> <li>분산 스토리지를 거치지 않고 처리를 계속하는 것</li> </ul> </li> <li>데이터 양이 너무 많아 분산 스토리지의 비용상, 성능상 한계를 넘어선다면 스트림 처리를 사용해 현실적인 흐름량을 줄일 수도 있음</li> <li>1초마다 통계량만 저장하고 싶으면 집계를 스트림 처리에 맡길 수도 있음</li> </ul> <p><br/></p> <h5 id="실시간-성이-높은-데이터-처리-시스템의-예">실시간 성이 높은 데이터 처리 시스템의 예</h5> <ul> <li>시스템 모니터링 <ul> <li>서버와 네트워크의 상태를 감시하고 그 시간 추이를 그래프로 표시</li> </ul> </li> <li>로그 관리 시스템 <ul> <li>운영체제의 시스템 이벤트나 로그 파일을 검색해 비정상적인 상태라면 경고를 생성</li> </ul> </li> <li>복합 이벤트 처리 <ul> <li>다수의 업무 시스템으로부터 보내온 이벤트를 자동으로 처리</li> </ul> </li> </ul> <p><br/></p> <h5 id="스트림-처리와-배치-처리의-차이">스트림 처리와 배치 처리의 차이</h5> <p><img src="/assets/images/batch_stream.JPG" alt="batch_stream" width="60%" height="60%"/></p> <ul> <li>데이터를 작게 분할해 DAG에서 실행한다는 점에서는 같음</li> <li>데이터의 처리가 끝나면 배치 처리는 종료, 반면 스트림 처리는 프로그램을 정지할 때까지 끝없이 실행이 계속됨</li> <li>하나의 프레임 워크에서 통합적인 데이터 처리를 기술할 수 있다는 점이 데이터 플로우의 장점</li> </ul> <p><br/></p> <h5 id="단어를-세는-spark-프로그램-스트림">단어를 세는 Spark 프로그램 (스트림)</h5> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 1초마다 스트림 처리를 한다.
</span><span class="n">sc</span> <span class="o">=</span> <span class="nc">SparkContext</span><span class="p">(</span><span class="sh">"</span><span class="s">local[2]</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">NetworkWordCount</span><span class="sh">"</span><span class="p">)</span>
<span class="n">ssc</span> <span class="o">=</span> <span class="nc">StreamingContext</span><span class="p">(</span><span class="n">sc</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="c1"># TCP 포트 9999로부터 데이터를 읽어들인다.
</span><span class="n">lines</span> <span class="o">=</span> <span class="n">ssc</span><span class="p">.</span><span class="nf">socketTextStream</span><span class="p">(</span><span class="sh">"</span><span class="s">localhost</span><span class="sh">"</span><span class="p">,</span> <span class="mi">9999</span><span class="p">)</span>
<span class="c1"># 파일의 각 행을 단어로 분해
</span><span class="n">words</span> <span class="o">=</span> <span class="n">lines</span><span class="p">.</span><span class="nf">flatMap</span><span class="p">(</span><span class="k">lambda</span> <span class="n">line</span><span class="p">:</span> <span class="n">line</span><span class="p">.</span><span class="nf">split</span><span class="p">())</span>
<span class="c1"># 단어별 개수를 콘솔에 출력
</span><span class="n">words</span><span class="p">.</span><span class="nf">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">word</span><span class="p">:</span> <span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span> \
     <span class="p">.</span><span class="nf">reduceByKey</span><span class="p">(</span><span class="k">lambda</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">:</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span><span class="p">)</span> \
     <span class="p">.</span><span class="nf">pprint</span><span class="p">()</span>
<span class="c1"># 스트림 처리를 시작한다.
</span><span class="n">ssc</span><span class="p">.</span><span class="nf">start</span><span class="p">()</span>
</code></pre></div></div> <p><br/></p> <h4 id="람다-아키텍처">람다 아키텍처</h4> <p><img src="/assets/images/lambda.JPG" alt="lambda" width="60%" height="60%"/></p> <ul> <li>스트림 처리는 잘못된 집계를 다시 하는 것이 어렵고, 늦게 전송된 데이터 취급이 어려우므로 배치 처리와 조합시켜 2계통의 데이터 처리를 하게 된다. <ul> <li>e.g.) 일별 보고서는 속보 값으로, 월별 보고서를 확정 값으로 분류</li> </ul> </li> <li>람다 아키텍처의 도입은 시스템을 복잡하게 하는 요인이 되므로, 꼭 필요하지 않은 경우 스트림 처리의 도입에는 신중해야 한다.</li> <li><strong>데이터 파이프라인을 3개의 레이어로 구분</strong> <ul> <li>모든 데이터는 반드시 <strong>배치 레이어</strong>에서 처리</li> <li>데이터 처리 결과는 <strong>서빙 레이어</strong>를 통해 접근 <ul> <li><strong>배치 뷰</strong>: 서빙 레이어에서 얻어진 결과, 정기적으로 업데이트 되고 실시간 정보는 얻을 수 없음</li> </ul> </li> <li><strong>스피드 레이어</strong> <ul> <li><strong>실시간 뷰</strong>: 스피드 레이어에서 얻은 결과</li> <li>실시간 뷰는 배치 뷰가 업데이트될 동안까지만 이용되고 오래된 데이터는 순서대로 삭제됨</li> </ul> </li> <li>배치 뷰와 실시간 뷰 모두를 조합시키는 형태로 쿼리를 실행</li> </ul> </li> <li>문제점: <strong>나쁜 개발 효율</strong> <ul> <li>스피드 레이어, 배치 레이어는 모두 똑같은 처리를 구현하고 있어 번거로움</li> </ul> </li> </ul> <p><br/></p> <h5 id="카파-아키텍처">카파 아키텍처</h5> <ul> <li>람다 아키첵처를 단순화함</li> <li>배치 레이어, 서빙 레이어를 제거하고 스피드 레이어만 남기는 대신 메시지 브로커의 데이터 보관 기간을 충분히 길게 하여 문제가 발생했을 때 메시지 배송 시간을 다시 과거로 설정한다.</li> <li>문제점: 부하가 높음 <ul> <li>스트림 처리의 데이터 플로우에 대량의 과거 데이터를 흘려 보매념 계산 자원을 일시적으로 과다하게 소비함</li> <li>클라우드 서비스 보급으로 이러한 자원 확보가 어렵지 않게 되어서 필요에 따라서는 스트림 처리를 다시 하는 것이 간단하다는 것이 카파 아키텍처의 생각</li> </ul> </li> </ul> <p><br/></p> <h4 id="빅데이터-분석-기반의-예">빅데이터 분석 기반의 예</h4> <p><img src="/assets/images/bigdata_flow.JPG" alt="bigdata_flow" width="60%" height="60%"/></p> <p><br/></p> <h4 id="aws-데이터-파이프라인-예시">AWS 데이터 파이프라인 예시</h4> <p><img src="/assets/images/aws.JPG" alt="aws" width="60%" height="60%"/></p> <p><br/></p> <h4 id="gcp-데이터-파이프라인-예시">GCP 데이터 파이프라인 예시</h4> <p><img src="/assets/images/gcp.JPG" alt="gcp" width="60%" height="60%"/></p>]]></content><author><name></name></author><category term="data-science"/><category term="data-engineering"/><category term="database"/><category term="data-science"/><summary type="html"><![CDATA[아래 대부분의 내용은 도서 빅데이터를 지탱하는 기술에서 발췌했습니다.]]></summary></entry><entry><title type="html">데이터 파이프라인의 자동화와 워크플로 관리</title><link href="https://seoyoungh.github.io/data-science/distribute-system-4/" rel="alternate" type="text/html" title="데이터 파이프라인의 자동화와 워크플로 관리"/><published>2020-08-12T00:00:00+00:00</published><updated>2020-08-12T00:00:00+00:00</updated><id>https://seoyoungh.github.io/data-science/distribute-system-4</id><content type="html" xml:base="https://seoyoungh.github.io/data-science/distribute-system-4/"><![CDATA[<p>아래 대부분의 내용은 도서 <strong>빅데이터를 지탱하는 기술</strong>에서 발췌했습니다.</p> <h4 id="워크플로-관리">워크플로 관리</h4> <ul> <li>기업 내의 정형적인 업무 프로세스를 진행하기 위한 구조</li> <li>자동화된 워크플로 <ul> <li>태스크는 정해진 스케줄에 따라 자동으로 실행되고, 무언가 비정상적 일이 발생한 경우에는 사람이 개입하여 문제를 해결</li> </ul> </li> </ul> <p><br/></p> <h4 id="워크플로-관리-도구">워크플로 관리 도구</h4> <ul> <li>정기적으로 태스크를 실행하고 비정상적인 상태를 감지하여 그것에 대한 해결을 돕는 역할 <ul> <li>태스크를 정기적인 스케줄로 실행하고 그 결과 통지</li> <li>태스크 간의 의존 관계를 정하고, 정해진 순서대로 빠짐없이 실행하기</li> <li>태스크의 실행 결과를 보관하고, 오류 발생 시엔 재실행할 수 있도록 함</li> </ul> </li> <li>데이터 파이프라인의 실행에 특화한 오픈 소스의 워크플로 관리 도구가 여럿 개발되었음</li> <li><strong>태스크</strong>: 데이터 파이프라인의 실행 과정에서 반복되는 개별 처리</li> </ul> <p><br/></p> <h5 id="워크플로-관리-도구의-종류">워크플로 관리 도구의 종류</h5> <ul> <li>선언형(declarative) 도구 <ul> <li>XML, YAML 등의 서식으로 워크플로를 기술</li> <li>미리 제공된 기능만 이용할 수 있고, <strong>유지 보수성</strong> 높음</li> <li>동일 쿼리를 파라미터만 바꿔 여러 번 실행하거나 워크플로를 단순 반복으로 자동 생성하는 경우에 유용</li> </ul> </li> <li>스크립트형(scripting) 도구 <ul> <li>스크립트 언어로 워크플로를 정의하는 유형</li> <li><strong>유연성</strong> 높음</li> </ul> </li> </ul> <p><br/></p> <h4 id="발생-가능한-오류의-복구-방법">발생 가능한 오류의 복구 방법</h4> <ul> <li>기본적으로 워크플로 관리에서는 오류에 대해 수작업에 의한 <strong>복구(recovery)</strong> 를 전제한 태스크를 설계</li> <li>실패한 태스크는 모두 기록하여 그것을 나중에 재실행할 수 있도록 함</li> <li><code class="language-plaintext highlighter-rouge">flow</code>: 워크플로 관리 도구에 의해 실행되는 일련의 태스크 <ul> <li>각 플로우에는 실행 시 고정 파라미터가 부여되어 있음 <ul> <li>e.g.) 일별 배칠 처리라면 특정 날짜가 파라미터가 -</li> </ul> </li> </ul> </li> <li><strong>복구의 기초</strong> <ul> <li>동일 플로우에 동일 파라미터를 건네면 완전히 동일한 태스크가 실행되도록 한다.</li> <li>플로우가 도중에 실패해도 나중에 동일 파라미터로 재실행이 가능하기 때문</li> <li>대부분의 워크플로 관리 도구는 과거에 실행한 플로우와 그 파라미터를 자동으로 데이터베이스에 기록하게 되어 있음</li> <li>실패한 플로우를 선택, 재실행하면 복구가 완료됨</li> </ul> </li> <li>태스크를 되도록 작게 유지해야 오류로 인한 스케줄 지연을 최소한으로 억제할 수 있음</li> <li><strong>재시도</strong> <ul> <li>여러 번 발생하는 오류에 대해 자동화해 수작업 없이 복구</li> <li>재시도 간격은 5분, 10분 정도로 둔다.</li> <li>재시도 횟수에는 주의가 필요 <ul> <li>재시도가 적으면 복구되기 전에 재시도가 종료되어 태스크 실행에 실패</li> <li>재시도가 너무 많으면 태스크가 실패하지 않은 것처럼 되기 때문에 중대한 문제가 발생해도 눈치채지 못함</li> </ul> </li> <li>이상적으로는 전혀 재시도 없이 모든 오류를 통지하는 것이 좋음</li> <li>재시도를 반복해도 문제가 없는 태스크라면, 1회나 2회의 재시도를 실행해도 좋지만, 오류의 원인을 그때마다 조사하여 오류가 일어나지 않도록 대책을 마련하는 것이 가장 좋음</li> </ul> </li> </ul> <p><br/></p> <h5 id="원자성-조작과-멱등한-조작">원자성 조작과 멱등한 조작</h5> <ul> <li>실패 또는 성공만 있어야 한다. 도중까지 성공은 허가하지 않음.</li> <li>데이터의 중복 등을 막기 위해 각 태스크가 시스템에 변경을 가하는 것을 한 번만 할 수 있도록 한다.</li> <li><strong>원자성 조작</strong> <ul> <li>쓰기가 필요한 수만큼 태스크를 나누도록 함</li> <li>하지만 여전히 재시도의 중복 가능성이 있음 <ul> <li>네트워크 경유로 로드 명령을 발행, 그 후에 통신이 끊어져 오류가 발생했다면 로드 명령이 취소될지 아니면 실행이 계속될지는 데이터베이스를 조작해봐야 안다.</li> </ul> </li> <li>재시도는 무효로 하고, 오류 발생 시에는 수작업으로 복구하는 편이 좋음</li> </ul> </li> <li><strong>멱등한 조작</strong> <ul> <li>동일한 태스크를 여러 번 실행해도 동일한 결과가 되도록 하는 것</li> <li>SQL의 경우, 테이블을 삭제한 후 다시 만들기</li> <li><strong>추가</strong>: 파일 업로드할 때마다 새로운 파일명을 만드는 경우 (데이터가 중복됨)</li> <li><strong>치환</strong>: 동일 파일명으로 덮어쓰기 (반복해도 결과가 변하지 않음, 멱등함) <ul> <li>멱등한 태스크를 만들기 위해서는 태스크에 부여된 파라미터를 이용해 고유의 이름을 생성하고, 여러 번 실행해도 항상 치환이 시행되도록 설계</li> <li>e.g.) 날짜와 시간을 파라미터로 전달함으로써 치환 형의 태스크를 구현</li> </ul> </li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">테이블 파티셔닝</code> <ul> <li>테이블을 1일 또는 1시간마다 파티션으로 분할하고, 파티션 단위로 치환하게 함</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">지수 백오프</code> <ul> <li>대다수의 데이터 전송 도구와 클라이언트 라이브러리는 재시도 횟수를 세세하게 제어하기 위한 옵션이 있음</li> <li>여러 시간 동안의 재시도가 예상되는 경우, 그 오류만을 대상으로 재시도 제어를 한다.</li> <li>재시도 횟수를 늘림과 동시에 조금씩 재시도 간격을 넓혀감</li> </ul> </li> </ul> <p><br/></p> <h4 id="처리-효율을-최대화하기">처리 효율을 최대화하기</h4> <ul> <li>워크플로 관리 도구는 외부 시스템에 영향을 미치는 부하를 조정하는 역할도 담당</li> <li>태스크의 크기나 동시 실행수를 잘 제어해 <strong>안정적인 태스크 실행</strong>, <strong>자원의 유효한 활용</strong> 을 양립하게 한다.</li> <li>워크플로 관리 도구에 등록된 태스크는 모두 너무 크지도 않고, 너무 작지도 않은 적당한 크기로 분할된 다수의 태스크가 여러 워커로부터 호출되고 있는 상태가 된다.</li> </ul> <p><br/></p> <h5 id="태스크-큐">태스크 큐</h5> <ul> <li>job queue, task queue</li> <li>너무 대량의 태스크를 동시 실행하면 서버에 과부하가 걸리므로, 제한을 두는 것</li> <li>모든 태스크를 일단 큐에 저장하고 일정 수의 워커 프로세스가 순서대로 꺼내면서 병렬화를 실현</li> </ul>]]></content><author><name></name></author><category term="data-science"/><category term="data-engineering"/><category term="database"/><category term="data-science"/><summary type="html"><![CDATA[아래 대부분의 내용은 도서 빅데이터를 지탱하는 기술에서 발췌했습니다.]]></summary></entry><entry><title type="html">데이터 수집 과정, 메시지 배송, 시계열 데이터</title><link href="https://seoyoungh.github.io/data-science/distribute-system-2/" rel="alternate" type="text/html" title="데이터 수집 과정, 메시지 배송, 시계열 데이터"/><published>2020-08-11T00:00:00+00:00</published><updated>2020-08-11T00:00:00+00:00</updated><id>https://seoyoungh.github.io/data-science/distribute-system-2</id><content type="html" xml:base="https://seoyoungh.github.io/data-science/distribute-system-2/"><![CDATA[<p>아래 대부분의 내용은 도서 <strong>빅데이터를 지탱하는 기술</strong>에서 발췌했습니다.</p> <p>벌크 형 &amp; 스트리밍 형의 데이터 전송, 시계열 데이터 수집을 최적화하는 방법에 대해 알아보자.</p> <p><br/></p> <h4 id="객체-스토리지와-데이터-수집">객체 스토리지와 데이터 수집</h4> <ul> <li>객체 스토리지(object storage) <ul> <li>Hadoop의 <code class="language-plaintext highlighter-rouge">HDFS</code>, <code class="language-plaintext highlighter-rouge">Amazon S3</code> 등</li> <li>다수의 컴퓨터를 사용해 파일을 여러 디스크에 복사해서 데이트의 중복화, 부하 분산을 실현</li> <li>객체 스토리지에서의 파일 I/O는 네트워크를 거쳐서 실행</li> <li>데이터 양이 많을 때에는 우수, 소량의 데이터에 대해서는 비효율적 (통신 오버헤드가 너무 크기 때문)</li> </ul> </li> <li>데이터 수집 <ul> <li>수집한 데이터를 가공하여 집계 효율이 좋게 분산 스토리지에 저장하는 프로세스</li> <li>시계열 데이터를 수시로 객체 스토리지에 기록하면 대량의 작은 파일이 생성되어 시간이 지남에 따라 성능을 저하시킴 <ul> <li>작은 데이터는 적당히 모아 하나의 큰 파일로 만들어 효율을 높이기</li> </ul> </li> <li>파일이 지나치게 커져도 네트워크 전송에 시간이 걸려 오류 발생률이 높아짐 <ul> <li>거대한 데이터는 한번에 처리하지 말고 적당히 나눠 문제 발생을 줄이기</li> </ul> </li> </ul> </li> </ul> <p><br/></p> <h4 id="벌크-형의-데이터-전송">벌크 형의 데이터 전송</h4> <ul> <li>전통적 데이터 웨어하우스에서 주로 사용되던 방식</li> <li>축적된 대량의 데이터가 이미 있는 경우나 기존의 데이터베이스에서 데이터를 추출하고 싶을 경우</li> <li>데이터 전송을 위한 <code class="language-plaintext highlighter-rouge">ETL 서버</code> 설치가 필요 <ul> <li>ETL 프로세스는 하루마다 또는 1시간 마다의 간격으로 정기적인 실행을 하므로 그동안 축적된 데이터는 하나로 모임</li> <li>한 번에 너무 적지도, 많지도 않은 데이터 양이 전송되도록 조정해야함</li> </ul> </li> <li>데이터 전송의 신뢰성이 중요한 경우에는 가능한 벌크 형 도구를 사용하기 <ul> <li>스트리밍 형의 데이터 전송은 나중에 재실행하기 쉽지 않음</li> <li>벌크 형의 경우, 문제가 발생하면 여러 번 데이터 전송을 재실행할 수 있다</li> </ul> </li> <li>벌크 형 데이터 전송은 주로 워크플로 관리 도구와 조합시켜 도입 <ul> <li>정기적인 스케줄 실행 및 오류 통지 등은 워크플로 관리 도구에 맡김</li> </ul> </li> </ul> <p><br/></p> <h4 id="스트리밍-형의-데이터-전송">스트리밍 형의 데이터 전송</h4> <ul> <li>계속되어 전송되어 오는 작은 데이터를 취급하기 위한 데이터 전송</li> <li><code class="language-plaintext highlighter-rouge">메시지 배송 message delivery</code> <ul> <li>다수의 클라이언트에서 계속해서 작은 데이터가 전송됨</li> <li>전송되는 데이터 양에 비해 통신을 위한 오버헤드가 커지므로 높은 성능의 서버가 요구됨</li> </ul> </li> <li>client: 메시지가 처음 생성되는 기기</li> <li>front-end: 메시지를 먼저 받는 서버 <ul> <li>역할: 통신 프로토콜을 제대로 구현하는 것, 메시지 브로커로 메시지를 전송</li> </ul> </li> <li>메시지 저장방법 <ol> <li>NoSQL에 데이터를 쓰고 Hive와 같은 쿼리 엔진으로 읽을 수 있음</li> <li>분산 스토리지에 직접 쓰는 것이 아니라, 메시지 큐와 메시지 브로커 등의 중계 시스템에 전송할 수 있음 <ul> <li>기록된 데이터는 일정한 간격으로 꺼내고 모아 함께 분산 스토리지에 저장</li> </ul> </li> </ol> </li> </ul> <p><br/></p> <h5 id="웹-브라우저에서의-메시지-배송">웹 브라우저에서의 메시지 배송</h5> <ul> <li>웹 서버 안에서 메시지를 만들어 배송</li> <li>전송 효율을 높이기 위해 <code class="language-plaintext highlighter-rouge">Fluentd</code>, <code class="language-plaintext highlighter-rouge">Logtash</code>와 같은 서버 상주형 로그 수집 소프트웨어를 활용 <ul> <li>내부의 효율적인 버퍼링 메커니즘으로 일정 시간동안 데이터를 축적해두었다가 모아서 보냄</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">웹 이벤트 트래킹 web event tracking</code> <ul> <li>JS를 사용하여 웹 브라우저에서 직접 메시지를 보내는 경우</li> <li>HTML 페이지에 태그를 삽입해 각종 액세스 분석, 데이터 분석 서비스 등에서 사용됨</li> </ul> </li> </ul> <p><br/></p> <h5 id="모바일-앱에서의-메시지-배송">모바일 앱에서의 메시지 배송</h5> <ul> <li>통신 방법으로 HTTP 프로토콜을 사용하는 클라이언트, 메시지 배송 방식이 웹 브라우저와 동일</li> <li><code class="language-plaintext highlighter-rouge">MBaas(Mobile Backend as a Service)</code> <ul> <li>서버를 직접 마련하지 않고 백엔드의 각종 데이터 저장 서비스를 이용할 수 있음</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">SDK</code> <ul> <li>서비스에서 제공되는 모바일 용 편리한 개발 kit</li> <li>모바일 앱에 특화된 액세스 해석 서비스를 통해 이벤트 데이터를 수집</li> <li>모바일 앱은 오프라인이 되는 경우도 많아 발생한 이벤트를 SDK 내부에 일단 축적시키고, 온라인 상태가 되었을 때 모아서 내보냄</li> </ul> </li> </ul> <p><br/></p> <h5 id="디바이스로부터의-메시지-배송">디바이스로부터의 메시지 배송</h5> <ul> <li>IoT</li> <li><code class="language-plaintext highlighter-rouge">MQTT</code> <ul> <li>TCP/IP를 사용해 데이터를 전송하는 프로토콜의 하나</li> <li>Pub/Sub 형 메시지 배송 <ul> <li>전달과 구독</li> <li>관리자에 의해 <code class="language-plaintext highlighter-rouge">topic</code>이 만들어지고, 이 topic을 구독하는 사람들은 메시지를 받는다. 메시지는 <code class="language-plaintext highlighter-rouge">MQTT 브로커</code>에 의해 구독자에게 전달된다.</li> <li>채팅 시스템, 메시징 앱, 푸시 알림 등의 시스템에서 자주 사용되는 기술</li> </ul> </li> </ul> </li> </ul> <p><br/></p> <h4 id="메시지-브로커">메시지 브로커</h4> <ul> <li>스토리지의 성능에 관계없이 안정적으로 대량의 메시지를 받도록 하는 중간층</li> <li>분산 스토리지에 직접 메시지를 기록하면 부하 제어가 어려워 성능 한계에 도달하기 쉬움</li> <li>메시지 브로커가 일시적으로 데이터를 축적함으로써 분산 스토리지에 쓰는 속도를 안정화함</li> <li><code class="language-plaintext highlighter-rouge">Apache Kafka</code>, <code class="language-plaintext highlighter-rouge">Amazon Kinesis</code></li> <li><code class="language-plaintext highlighter-rouge">메시지 라우팅</code> <ul> <li>메시지 브로커에 써넣은 데이터를 복수의 다른 소비자가 읽어들이게 해 메시지를 복사, 데이터를 여러 경로로 분기시키는 것</li> </ul> </li> <li>메시지 배송의 안정성을 높이는데 유용하지만, 메시지 브로커 자체에 장애가 일어날 수 있으므로 방심하면 안 된다.</li> </ul> <p><br/></p> <h4 id="메시지-배송의-신뢰성">메시지 배송의 신뢰성</h4> <ul> <li><code class="language-plaintext highlighter-rouge">신뢰성 reliability</code> <ul> <li> <p>모바일 회선과 같은 신뢰성이 낮은 네트워크에서는 반드시 메시지의 중복이나 누락이 발생</p> </li> <li> <p>처리 방법: 다음 중 하나를 보장하도록 설계됨</p> <ol> <li>at most once <ul> <li>메시지는 한 번만 전송됨, 도중에 전송 실패로 결손 발생 가능성이 있음</li> <li>무슨 일이 일어나도 절대로 메시지를 다시 보내지 않음</li> <li>그러나 대개는 재전송이 이루어지므로 at most once를 보장하는 것은 어려움</li> </ul> </li> <li>exactly once <ul> <li>메시지는 손실되거나 중복 없이 한 번만 전달됨</li> <li>메시지 송신/수신 측 모두 서로의 정보를 코디네이터에게 전달함으로써 문제 발생시 코디네이터의 지시에 따라 해결</li> <li>코디네이터가 존재하지 않으면 실현되지 못함</li> </ul> </li> <li><strong>at least once</strong> <ul> <li>메시지는 확실히 전달되지만, 중복 가능성이 있음</li> <li>대부분 이를 보장하되, 중복 제거는 사용자에게 맡김</li> </ul> </li> </ol> </li> </ul> </li> <li>신뢰성이 높은 메시지 배송을 실현하려면, 중간 경로를 모두 <code class="language-plaintext highlighter-rouge">at least once</code>로 통일하고, 클라이언트 상에서 모든 메시지에 고유 ID를 포함하도록 하고, <strong>경로의 말단에서 중복 제거를 실행해야 한다.</strong></li> </ul> <p><br/></p> <h4 id="데이터-수집의-파이프라인">데이터 수집의 파이프라인</h4> <p><img src="/assets/images/stream_pipeline.JPG" alt="stream_pipeline" width="60%" height="60%"/></p> <ul> <li>요구 사항에 맞게, 필요에 따라 시스템을 조합</li> </ul> <p><br/></p> <h4 id="시계열-데이터-수집의-문제점">시계열 데이터 수집의 문제점</h4> <ul> <li>스마트폰에서 데이터를 수집하면 오프라인 상태, 방전 등의 이유로 메시지가 지연되는 경우가 흔하다.</li> <li><code class="language-plaintext highlighter-rouge">event time</code>: 클라이언트 상에서 메시지가 생성된 시간</li> <li><code class="language-plaintext highlighter-rouge">process time</code>: 서버가 처리하는 시간</li> <li>며칠 정도의 지연을 예측해 데이터 분석을 고려해야 한다. <ul> <li>e.g.) 모바일 앱의 활동 사용자 수를 집계할 때 이벤트 시간보다 며칠 정도 지난 시점에서 소급해 집계해야 함</li> </ul> </li> <li>분산 스토리지에 데이터를 넣는 단계에서는 프로세스 시간을 사용하는 것이 보통 <ul> <li>이벤트 시간으로 정렬되지 않기 때문에 모든 데이터를 로드해, 원하는 이벤트 시간을 포함하는지 확인해야 함 - 풀 스캔 발생</li> <li><code class="language-plaintext highlighter-rouge">full scan</code>: 다수의 파일을 모두 검색하는 쿼리, 시스템의 부하를 크게 높이는 요인</li> </ul> </li> </ul> <p><br/></p> <h5 id="시계열-데이터-최적화">시계열 데이터 최적화</h5> <ol> <li>시계열 인덱스 <ul> <li>이벤트 시간에 대해 인덱스를 만드는 것</li> <li>Cassandra와 같이 시계열 인덱스에 대응하는 분산 데이터베이스를 이용하면 처음부터 이벤트 시간으로 인덱스 된 테이블을 만들 수 있다.</li> <li>하지만, 장기간에 걸쳐 대량의 데이터를 집계하는 경우에는 분산 데이터베이스가 그다지 효율적이지 못함. 열 지향 스토리지를 지속적으로 만들어야 함</li> </ul> </li> <li>조건절 푸쉬다운 <ul> <li><code class="language-plaintext highlighter-rouge">predicate pushdown</code></li> <li>통계를 이용하여 최소한의 데이터만을 읽도록 하는 최적화</li> <li>열 지향 스토리지를 만들 때 가급적 읽어들이는 데이터의 양을 최소화하도록 데이터를 정렬해둠</li> <li>이 때, 데이터가 연속적으로 배치되어야 최적화 효과가 높아짐</li> </ul> </li> <li><strong>데이터 마트를 이벤트 시간으로 정렬하기</strong> <ul> <li>데이터 마트만이 이벤트 시간에 의한 정렬을 고려하도록 해두는 것</li> <li>데이터 수집 단계에서는 이벤트 시간을 따지지 않고 프로세스 시간만을 사용하여 데이터를 저장</li> </ul> </li> </ol>]]></content><author><name></name></author><category term="data-science"/><category term="data-engineering"/><category term="database"/><category term="data-science"/><summary type="html"><![CDATA[아래 대부분의 내용은 도서 빅데이터를 지탱하는 기술에서 발췌했습니다.]]></summary></entry><entry><title type="html">NoSQL 데이터베이스 구체적으로 알아보기</title><link href="https://seoyoungh.github.io/data-science/distribute-system-3/" rel="alternate" type="text/html" title="NoSQL 데이터베이스 구체적으로 알아보기"/><published>2020-08-11T00:00:00+00:00</published><updated>2020-08-11T00:00:00+00:00</updated><id>https://seoyoungh.github.io/data-science/distribute-system-3</id><content type="html" xml:base="https://seoyoungh.github.io/data-science/distribute-system-3/"><![CDATA[<p>아래 대부분의 내용은 도서 <strong>빅데이터를 지탱하는 기술</strong>에서 발췌했습니다.</p> <p>ACID, CAP, NoSQL 데이터베이스의 종류와 예시에 대해 알아보자.</p> <h4 id="객체-스토리지">객체 스토리지</h4> <ul> <li>임의의 파일을 저장할 수 있는 장점이 있지만, 객체 스토리지 상의 파일은 교체하기 어려움</li> <li>데이터베이스처럼 수시로 변경하는 용도로는 적합하지 않음</li> <li>쓰기 빈도가 높은 데이터는 별도 RDB에 저장하고 정기적으로 스냅샷을 하거나, 다른 분산 데이터베이스에 저장하도록 한다.</li> </ul> <p><br/></p> <h4 id="acid">ACID</h4> <ul> <li>Atomicity: 원시성</li> <li>Consitency: 일관성</li> <li>Isolation: 독립성</li> <li>Durability: 내구성</li> <li>일반적인 RDB는 이를 충족해 신뢰성 있는 트랜잭션 처리를 하고 있음</li> </ul> <p><br/></p> <h4 id="cap-정리">CAP 정리</h4> <ul> <li>ACID 특성의 한계에서 제창된 것</li> <li>Consitency: 일관성</li> <li>Availability: 가용성</li> <li>Partition-Tolerance: 분단내성</li> </ul> <p><br/></p> <h4 id="nosql-데이터-베이스">NoSQL 데이터 베이스</h4> <ul> <li>특정 용도에 최적화된 데이터 저장소</li> <li>애플리케이션에서 처음에 데이터를 기록하는 장소로 이용</li> <li>데이터를 집계하는 데는 적합하지 않고, 데이터를 분석하려면 데이터를 추출해야함</li> <li>NoSQL 데이터 베이스 중 일부는 CAP 정리의 일관성이나 가용성 중 하나를 선택함 <ul> <li>일관성 우선 가용성 포기: 단시간의 장애 발생을 수용</li> <li>가용성 우선 일관성 포기: 오래된 데이터를 읽을 수 없음</li> </ul> </li> <li>NoSQL 데이터 베이스는 기존 RDB의 한계를 뛰어 넘기 위해 개발되어와서 ACID 특성을 부분적으로 포기하는 경우가 많다. 각 데이터베이스의 제약을 잘 이해두어야함</li> <li><code class="language-plaintext highlighter-rouge">결과 일관성(eventual consistency)</code> <ul> <li>써넣은 데이터를 바로 읽을 수 있다곤 말할 수 없음</li> <li>시간이 지나면 언젠가 최신 데이터를 읽을 수 있음을 보장하지만, 그것이 언제가 될지는 알 수 없음</li> <li>Amazon DynamoDB, Amazon S3에서 도입되고 있음</li> </ul> </li> </ul> <p><br/></p> <h4 id="분산-kvs">분산 KVS</h4> <ul> <li>Distributed Key-Value Store</li> <li>모든 데이터를 키값 쌍으로 저장하도록 설계된 데이터 저장소</li> <li>몇 kb 정도의 데이터를 초당 수만번 읽고 쓰는 경우</li> <li>모든 데이터에 고유의 키를 지정하고 그것을 부하 분산을 위해 이용</li> <li>키가 정해지면 그 값을 클러스터의 어느 노드에 배치할 것인지 결정</li> <li>이 구조에 의해 노드 간에 부하를 균등하게 분산하고 노드를 증감하는 것만으로 클러스터의 성능을 변경할 수 있게 되어있음</li> </ul> <p><br/></p> <h5 id="분산-kvs-아키텍처">분산 KVS 아키텍처</h5> <ol> <li>마스터/슬레이브 형 <ul> <li>1대의 마스터가 전체를 관리, 마스터가 중지되면 아무도 읽고 쓸 수 없음</li> </ul> </li> <li>P2P 형 <ul> <li>모든 노드가 대등한 관계, 클라이언트는 어떤 노드에 연결해도 데이터를 읽고 쓸 수 있음</li> </ul> </li> </ol> <p><br/></p> <h5 id="amazon-dynamodb">Amazon DynamoDB</h5> <ul> <li>항상 안정된 읽기 쓰기 성능을 제공하도록 디자인된 분산형 NoSQL 데이터베이스</li> <li>P2P형 분산 아키텍처</li> <li>미리 설정한 초 단위의 요청 수에 따라 노드가 증감된다.</li> <li>데이터의 읽기 및 쓰기에 지연이 발생하면 곤란한 애플리케이션에 유용</li> <li>데이터를 분석하려면, AWS 서비스인 Amazon EMR 및 Amazon Redshift 등과 결합해 Hive에 의한 배치 처리를 실행하거나 데이터 웨어하우스에 데이터를 전송하도록 한다.</li> </ul> <p><br/></p> <h4 id="와이드-컬럼-스토어">와이드 컬럼 스토어</h4> <ul> <li>Wide-column store</li> <li>분산 KVS를 발전시켜 2개 이상의 임의의 키에 데이터를 저장할 수 있도록 한 것</li> <li><code class="language-plaintext highlighter-rouge">Google Cloud Bigtable</code>, <code class="language-plaintext highlighter-rouge">Apache HBase</code>, <code class="language-plaintext highlighter-rouge">Apache Cassandra</code></li> <li>행만이 아니라 열도 계속 증가함</li> <li>키마다 컬럼과 값이 저장되는 중첩의 데이터 구조로 되어있음</li> <li>주로 성능 향상을 목표로 함</li> </ul> <p><br/></p> <h5 id="apache-cassandra">Apache Cassandra</h5> <ul> <li>내부 데이터 저장소로 와이드 컬럼 스토어 이용</li> <li>CQL이라 불리는 높은 수준의 쿼리 언어가 구현되어 있음</li> <li>테이블의 스키마를 결정할 필요가 있기 때문에 구조화 데이터만을 취급할 수 있음</li> <li>P2P형 분산 아키텍처</li> </ul> <p><br/></p> <h4 id="도큐먼트-스토어">도큐먼트 스토어</h4> <ul> <li>Document store</li> <li>주로 데이터 처리의 유연성을 목적으로 함</li> <li>JSON처럼 복잡하게 뒤얽힌 스키마리스 데이터를 그대로의 형태로 저장하고 쿼리를 실행할 수 있도록 함</li> <li>스키마를 정하지 않고 데이터 처리를 할 수 있으므로 외부에서 들여온 데이터를 저장하는데 특히 적합 <ul> <li>참고 시스템의 데이터 및 로그 저장에 적합</li> </ul> </li> <li>최근 MySQL, PostgreSQL과 같은 RDB에서도 도큐먼트 스토어의 기능이 포함됨</li> </ul> <p><br/></p> <h5 id="mongodb">MongoDB</h5> <ul> <li>JS나 각종 프로그래밍 언어를 사용해 데이터를 읽고 쓸 수 있음</li> <li>성능을 우선하여 신뢰성을 희생한다는 비판에도 불구, 간편함 덕에 인기가 많음</li> <li>데이터 분석이 목적인 경우, 쿼리 엔진으로부터 접속하는 등 데이터를 추출할 필요가 있음</li> </ul> <p><br/></p> <h4 id="검색-엔진">검색 엔진</h4> <ul> <li>저장된 데이터를 독자적인 쿼리 언어로 찾아냄</li> <li>텍스트 데이터를 전문 검색하기 위해 inverted index를 만들어 키워드 검색을 고속화함</li> <li><code class="language-plaintext highlighter-rouge">Elasticsearch</code>, <code class="language-plaintext highlighter-rouge">Splunk</code></li> </ul>]]></content><author><name></name></author><category term="data-science"/><category term="data-engineering"/><category term="database"/><category term="data-science"/><summary type="html"><![CDATA[아래 대부분의 내용은 도서 빅데이터를 지탱하는 기술에서 발췌했습니다.]]></summary></entry><entry><title type="html">Hadoop, Hive, Spark에 대해 자세히 알아보기</title><link href="https://seoyoungh.github.io/data-science/distribute-system-1/" rel="alternate" type="text/html" title="Hadoop, Hive, Spark에 대해 자세히 알아보기"/><published>2020-08-10T00:00:00+00:00</published><updated>2020-08-10T00:00:00+00:00</updated><id>https://seoyoungh.github.io/data-science/distribute-system-1</id><content type="html" xml:base="https://seoyoungh.github.io/data-science/distribute-system-1/"><![CDATA[<p>아래 대부분의 내용은 도서 <strong>빅데이터를 지탱하는 기술</strong>에서 발췌했습니다.</p> <p>Schema란 무엇인지, 그리고 다양한 분산 처리 시스템에 대해 알아보자.</p> <p><br/></p> <h4 id="데이터의-종류">데이터의 종류</h4> <ul> <li><strong>Schema</strong> <ul> <li>테이블의 컬럼 이름, 데이터 타입, 테이블 간의 관계 등을 정의한 것</li> </ul> </li> <li>Structured data <ul> <li>스키마가 명확하게 정의된 데이터</li> <li>table</li> </ul> </li> <li>Unstructured data <ul> <li>스키마가 없는 데이터</li> <li>텍스트 데이터, 이미지, 동영상 등의 미디어 데이터</li> <li>SQL로 제대로 집계할 수 없음, 데이터 가공하는 과정에서 스키마를 정의, 구조화된 데이터로 변환시킬 수 있음</li> </ul> </li> <li>Schemaless data <ul> <li>기본 서식은 있지만, 스키마가 정의되지 않은 데이터</li> <li>csv, json, xml 등 <ul> <li>엄밀히 말하면 json, xml은 Semi-Structured data</li> </ul> </li> </ul> </li> </ul> <p><br/></p> <h4 id="열-지향-스토리지로의-변환">열 지향 스토리지로의 변환</h4> <ul> <li>비구조화 데이터는 열 지향 스토리지로 변환이 필요하다.</li> <li>Hadoop에서 사용하는 열 지향 스토리지 <ul> <li>Apache ORC: 구조화 데이터를 위한 열 지향 스토리지</li> <li>Apache Parquet: 스키마리스에 가까운 데이터 구조, json 파일도 저장 가능</li> </ul> </li> <li>열 지향 스토리지로 변환하기 위해서는 데이터의 가공 및 압축을 위해 많은 컴퓨터 리소스가 소비된다.</li> <li>이때 사용되는 것이 Hadoop과 Spark 등의 분산 처리 프레임워크!</li> </ul> <p><br/></p> <h4 id="hadoop-ecosystem">Hadoop Ecosystem</h4> <ul> <li>하둡은 단일 소프트웨어가 아닌 분산 시스템을 구성하는 다수의 소프트웨어로 이루어진 집합체</li> <li>리소스 관리자 YARN 상에서 복수의 분산 애플리케이션이 동작하는 구성</li> <li>대규모 분산 시스템을 구축하기 위한 공통 플랫폼의 역할</li> <li>모든 분산 시스템을 하둡에 의존하지 않고, 일부만 사용할 수 있다. <ul> <li>e.g.) HDFS를 사용하면서 리소스 관리자는 <code class="language-plaintext highlighter-rouge">Mesos</code>, 분산 데이터 처리에는 <code class="language-plaintext highlighter-rouge">Spark</code>를 사용하는 구성</li> </ul> </li> </ul> <p><br/></p> <h5 id="구성-요소">구성 요소</h5> <ul> <li><code class="language-plaintext highlighter-rouge">HDFS</code>: 분산 파일 시스템 <ul> <li>하둡에서 처리되는 데이터 대부분은 HDFS에 저장된다.</li> <li>데이터가 항상 여러 컴퓨터에 복사되도록 함</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">YARN</code>: 리소스 매니저 <ul> <li>CPU나 메모리등의 계산 리소스를 계산</li> <li>애플리케이션이 사용하는 CPU 코어와 메모리를 <code class="language-plaintext highlighter-rouge">container</code> 단위로 관리</li> <li>클러스터 전체의 부하를 보고 비어있는 호스트로부터 애플리케이션에 컨테이너를 할당</li> <li>중요하지 않은 배치 처리에는 낮은 우선순위를 부여하는 등 애플리케이션마다 실행의 우선순위를 정할 수 있음</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">MapReduce</code>, <code class="language-plaintext highlighter-rouge">Tez</code>, <code class="language-plaintext highlighter-rouge">Hive</code>: 분산 데이터 처리 <ul> <li><code class="language-plaintext highlighter-rouge">MapReduce</code> <ul> <li>임의의 자바 프로그램을 실행시킬 수 있으므로 비구조화 데이터 가공에 적합</li> <li>작은 프로그램을 실행하려면 오버헤드가 매우 큼</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">Hive</code>: 쿼리 엔진 <ul> <li>SQL 등 쿼리 언어에 의한 데이터 집계가 목적이라면 쿼리 엔진을 사용해야 함</li> <li>쿼리를 자동으로 MapReduce 프로그램으로 변환하는 소프트웨어</li> <li>MapReduce를 계승했기 때문에 여러 번의 애드 혹 쿼리 실행이 아닌 시간이 걸리는 배치 처리에 적합</li> <li>Hive에서 만든 각 테이블의 정보는 <code class="language-plaintext highlighter-rouge">Hive 메타 스토어</code>라 불리는 특별한 DB에 저장됨</li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">Tez</code>: Hive를 가속화하기 위한 노력 <ul> <li>기존의 MapReduce의 단점을 해결, 고속화를 실현</li> <li>불필요한 단계가 감소하여 처리가 짧아지고 스테이지 사이의 대기 시간이 없어 처리 전체가 동시에 실행되어 실행시간이 단축됨</li> </ul> </li> </ul> </li> </ul> <p><br/></p> <h4 id="대화형-쿼리-엔진">대화형 쿼리 엔진</h4> <ul> <li><code class="language-plaintext highlighter-rouge">Impala</code>, <code class="language-plaintext highlighter-rouge">Presto</code></li> <li>Hive를 고속화하는 것이 아니라 처음부터 <strong>대화형의 쿼리 실행만 전문으로 하는 쿼리 엔진</strong></li> <li><strong>MapReduce, Hive, Tez는 장시간의 배치 처리를 가정</strong>, 한정된 리소스를 유효하게 활용하도록 설계되어 있음</li> <li>대화형 쿼리 엔진은 순간 최대 속도를 높이기 위해 모든 오버헤드가 제거되어 사용할 수 있는 리소스를 최대한 활용하여 쿼리를 실행</li> </ul> <p><br/></p> <h5 id="presto">Presto</h5> <ul> <li>하나의 코디네이터와 여러 workers로 구성됨 <ul> <li>CLI 등의 클라이언트에서 코디네이터로 쿼리 전송, 코디네이터는 쿼리를 분석하고 실행 계획을 수립해 worker에게 처리를 분배</li> </ul> </li> <li>Presto는 전용 스토리지를 갖고 있지 않으므로, 다양한 데이터 소스에서 직접 데이터를 읽어 들임</li> <li>성능을 발휘하기 위해서는 원래 스토리지가 열 지향 데이터 구조로 되어 있어야 한다.</li> <li>Hive에서 만든 구조화 데이터를 좀 더 집계하는 등의 목적에 적합 <ul> <li>Hive 메타 스토어에 등록된 테이블을 가져올 수 있다.</li> </ul> </li> <li>하나의 쿼리 안에서 분산 스토리지의 팩트 테이블과 MySQL의 마스터 테이블을 조인할 수도 있음</li> <li>쿼리의 실행 과정에서 디스크에 쓰기를 하지 않음</li> <li><code class="language-plaintext highlighter-rouge">분산 결합(distribute join)</code> <ul> <li>같은 키를 갖는 데이터는 동일한 노드에 모임</li> <li>노드 간의 데이터 전송을 위한 네트워크 통신이 발생하기 때문에 종종 쿼리의 지연을 초래</li> <li>한쪽 테이블이 충분히 작은 경우에는 <code class="language-plaintext highlighter-rouge">브로드캐스트 결합(broadcast join)</code>을 사용하여 처리 속도를 크게 고속화 할 수 있음 <ul> <li>결합하는 테이블의 모든 데이터가 각 노드에 복사됨</li> </ul> </li> </ul> </li> </ul> <p><br/></p> <h5 id="쿼리-엔진-활용-방법">쿼리 엔진 활용 방법</h5> <ul> <li>대량의 비구조화 데이터를 가공하는 무거운 배치 처리에는 높은 처리량으로 리소스를 활용하는 <code class="language-plaintext highlighter-rouge">Hive</code></li> <li>그렇게 완성된 구조화 데이터를 대화식으로 집계할 때는 지연이 적은<code class="language-plaintext highlighter-rouge">Impala</code>, <code class="language-plaintext highlighter-rouge">Presto</code></li> </ul> <p><br/></p> <h4 id="hive-효율적으로-사용하기">Hive 효율적으로 사용하기</h4> <ul> <li>Hive는 데이터베이스가 아닌 <strong>데이터 처리를 위한 배치 처리 구조</strong></li> <li>읽어 들이는 데이터의 양을 의식하면서 쿼리를 작성해야 원하는 성능이 나올 수 있음 <ul> <li>가능한 의식을 해서 sub query 안에서 fact table을 작게 하도록 해서 중간 데이터를 줄여야 함</li> <li>그냥 JOIN하게 되면 매우 거대한 중간 데이터를 만들고, 메모리를 낭비할 수 있음</li> </ul> </li> <li>데이터의 편향을 피해야함 <ul> <li>데이터의 편차(data skew)는 고속화를 방해함</li> <li>분산 시스템의 성능을 발휘하기 위해서는 데이터의 편차를 최대한 없애고, 모든 노드에 데이터가 균등하게 분산되도록 해야 함</li> <li>중복을 제거하면 부하를 분산시킬 수 있음</li> </ul> </li> </ul> <p><br/></p> <h4 id="spark">Spark</h4> <ul> <li>분산 시스템을 사용한 프로그래밍 환경</li> <li>Hadoop과는 다른 독립된 프로젝트, Hadoop이 아닌 MapReduce를 대체하는 것 <ul> <li>HDFS나 YARN을 사용하면서 쓸 수도 있고 아예 Hadoop을 사용하지 않을 수도 있다.</li> <li>분산 스토리지 Amazon S3를 이용하거나, 분산 DB 카산드라에서 데이터를 읽어들이는 것도 가능</li> </ul> </li> <li>대량의 메모리를 활용하여 고속화를 실현하는 것 <ul> <li>기존엔 가용 메모리가 적었기 때문에 MapReduce는 처리의 대부분을 디스크 I/O에 사용</li> <li>컴퓨터의 메모리 양이 증가하면서, 디스크 I/O를 줄이고 <strong>가능한 많은 데이터를 메모리상에 올린 상태로 두어 디스크에는 아무것도 기록하지 않는다.</strong></li> <li>중간 데이터를 디스크에 쓰지 않고 메모리에 보존, 프로그램 실행 중에는 많은 메모리가 필요하지만 실행시간은 단축됨</li> <li>장애로 중간 데이터를 잃어버려도 한 번 더 입력 데이터로 다시 실행</li> <li>의도적으로 디스크 상에 캐시하는 것도 가능</li> <li><strong>메모리 관리가 중요, 메모리를 개발자가 제어할 수 있어야함</strong></li> </ul> </li> <li>Spark 상의 데이터 처리는 스크립트 언어를 사용할 수 있다 (자바, 스칼라, 파이썬, R 등)</li> <li>대규모 배치 처리 뿐만이 아니라 SQL에 의한 대화형 쿼리 실행과 실시간 스트림 처리까지 널리 이용됨</li> <li>Hive의 데이터 구조화, Presto에 의한 SQL 실행을 Spark에서는 하나의 스크립트 안에서 실행할 수 있음</li> </ul>]]></content><author><name></name></author><category term="data-science"/><category term="data-engineering"/><category term="database"/><category term="data-science"/><summary type="html"><![CDATA[아래 대부분의 내용은 도서 빅데이터를 지탱하는 기술에서 발췌했습니다.]]></summary></entry></feed>