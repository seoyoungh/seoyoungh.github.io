<!DOCTYPE html> <html lang="en"> <head> <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        TeX: {
          equationNumbers: {
            autoNumber: "AMS"
          }
        },
        tex2jax: {
        inlineMath: [ ['$', '$'] ],
        displayMath: [ ['$$', '$$'] ],
        processEscapes: true,
      }
    });
    MathJax.Hub.Register.MessageHook("Math Processing Error",function (message) {
          alert("Math Processing Error: "+message[1]);
        });
    MathJax.Hub.Register.MessageHook("TeX Jax - parse error",function (message) {
          alert("Math Processing Error: "+message[1]);
        });
    </script> <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Linear Regresssion | Seoyoung Hong</title> <meta name="author" content="Seoyoung Hong"/> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "/> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light"/> <link rel="shortcut icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>⚛️</text></svg>"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://seoyoungh.github.io/deep-learning-zero-to-all/zerotoall-2/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Seoyoung </span>Hong</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Linear Regresssion</h1> <p class="post-meta">August 19, 2019</p> <p class="post-tags"> <a href="/blog/2019"> <i class="fas fa-calendar fa-sm"></i> 2019 </a>   ·   <a href="/blog/tag/deep-learning"> <i class="fas fa-hashtag fa-sm"></i> deep-learning</a>   <a href="/blog/tag/tensorflow"> <i class="fas fa-hashtag fa-sm"></i> tensorflow</a>     ·   <a href="/blog/category/deep-learning-zero-to-all"> <i class="fas fa-tag fa-sm"></i> deep-learning-zero-to-all</a>   </p> </header> <article class="post-content"> <p><code class="language-plaintext highlighter-rouge">Sung Kim 교수님 강의로 공부하는 머신러닝 Section 2,3</code></p> <h3 id="linear-regression">Linear regression</h3> <p>x와 y가 <code class="language-plaintext highlighter-rouge">linear relationship</code>을 가진다고 판단이 들 때, 일차방정식 형태의 가설을 세워 모델링하는 것이다.</p> <p><br></p> <h4 id="hypothesis">Hypothesis</h4> <p>$H(x)=Wx+b$ 가설은 일차방정식의 형태를 띈다. W는 weight, slope(기울기)이고 b는 bias, intercept(절편)이다.<br> <code class="language-plaintext highlighter-rouge">Simplified hypothesis</code>: 이를 간소화해서 $H(x)=Wx$로 나타내기도 한다.</p> <p><br></p> <h3 id="residual잔차">Residual(잔차)</h3> <p>잔차는 $H(X)-y$, 모델에서 예측한 값 - 실제 예측한 값이다.</p> <p><br></p> <h3 id="rssresidual-sum-of-squares">RSS(residual sum of squares)</h3> <p>잔차는 음수가 될 수도, 양수가 될 수도 있기 때문에 square처리해서 모든 잔차를 더한다. RSS를 식으로 나타내면 $\sum_{i=1}^m (H(X_i)-y_i)^2$이다.</p> <p><br></p> <h3 id="cost-function">Cost function</h3> <p>$cost(W,b) = 1/m * \sum_{i=1}^m (H(X_i)-y_i)^2$</p> <p>좋은 모델을 위해서는 잔차를 최소화 해야한다. 이 잔차 합의 평균을 구하는 함수를 <code class="language-plaintext highlighter-rouge">Cost function</code>이라고 부르는데, <strong>이 cost를 minimize 하는 w,b를 찾아야한다.</strong> 이 식은 결국 모델의 Total Error를 보여준다고 할 수 있겠다. 이 식은 <code class="language-plaintext highlighter-rouge">Mean Squared Error</code>라고도 불린다.</p> <p><code class="language-plaintext highlighter-rouge">Simplified cost function</code>: bias를 고려하지 않고 $cost = 1/m * \sum_{i=n}^m (W(X^i)-y^i)^2$로 나타내기도 한다. Cost function은 <code class="language-plaintext highlighter-rouge">Loss function</code>이라고도 불린다.</p> <p><br></p> <h3 id="gradient-descent-algorithm경사-하강법">Gradient descent algorithm(경사 하강법)</h3> <p>Cost function을 minimize하는 w,b를 찾아내는 알고리즘</p> <p><img src="/assets/images/gda.png" alt="gda" width="60%" height="60%"></p> <p><code class="language-plaintext highlighter-rouge">Gradient</code>는 <code class="language-plaintext highlighter-rouge">경사</code>를, <code class="language-plaintext highlighter-rouge">Descent</code>는 <code class="language-plaintext highlighter-rouge">내려간</code>을 의미한다. 경사도를 따라서 내려가다가 경사도가 0이 되었을 때 stop하는 알고리즘이라고 할 수 있다.</p> <ul> <li>0,0에서 시작 (또는 다른 값에서 시작)</li> <li>W,b 을 조금 바꾸고 줄일 수 있는지 확인한다.</li> <li>파라미터 (W,b)를 바꿀때마다 cost(W,b)를 가장 줄일수 있는 경사도 (gradient)를 선택한다.</li> <li>미분을 이용해 구하는데, tensorflow에서 자체적으로 구해주기 때문에 걱정 안해도 된다.</li> </ul> <p>W := W - a(cost(W)를 미분한 값)</p> <ul> <li>descent가 +이면, -로 이동, descent가 -이면, +로 이동</li> <li>기울기가 음수이면 W의 변화량이 양수가 되므로 오른쪽으로 이동, 기울기가 양수이면 W의 변화량이 음수가 되므로 왼쪽으로 이동</li> </ul> <p>여기서 a(alpha)는 learning rate라고 불리는, step을 조절하는 값이다.</p> <p>Learning rate를 적절하게 설정해야 하는 이유는 다음과 같다.</p> <ol> <li>Learning rate가 너무 크면, 발산해버리는 경우가 발생한다. (Overshooting)</li> <li>Learning rate가 너무 작으면, 일단 수렴하는 속도가 너무 느리고, local minimum에 빠질 확률이 증가한다.</li> </ol> <p><strong>이것이 training의 원리이다.</strong></p> <ol> <li>random weights로 training을 시작하고.</li> <li>기울기의 반대 방향으로 weights를 수정하며 training 시킨다.</li> </ol> <p><br></p> <h3 id="delta-rule">Delta Rule</h3> <p>The delta rule learning formula for weight adjustment.</p> \[∆𝑤 = c(t_i-f(net_j))f'(net_j)x_k\] <ul> <li>$c$ = learning rate</li> <li>$t_i$ = desired output</li> <li>$f(x)$ = actual output values of the $i_\text{th}$ node</li> <li>$f’$ = The derivative</li> <li>$x_k$ = the $k_\text{th}$ input to node i</li> </ul> <p><a href="http://blog.naver.com/PostView.nhn?blogId=infoefficien&amp;logNo=220906916685&amp;parentCategoryNo=&amp;categoryNo=529&amp;viewDate=&amp;isShowPopularPosts=true&amp;from=search" target="_blank" rel="noopener noreferrer">델타 규칙에 대해 잘 설명된 블로그 글을 첨부한다.</a></p> <p><br></p> <h3 id="convex-function">Convex function</h3> <p>시작점에 상관없이 도착점이 같은 function</p> <p>Cost function은 convex function이여야 한다. 확인하고 GDA를 사용해야한다! 하지만 다행히 우리 hypothesis가 convex function이기 때문에 늘 minimum을 찾을 수 있다.</p> </article> <script src="https://utteranc.es/client.js" repo="seoyoungh/blog-comments" issue-term="pathname" theme="github-light" crossorigin="anonymous" async> </script> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2024 Seoyoung Hong. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="noopener noreferrer">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-453QGGY7P1"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-453QGGY7P1");</script> </body> </html>