---
layout: post
title:  "Linear Regresssion"
date: 2019-08-19
use_math: true
categories:
  - deep-learning-zero-to-all
tags:
  - deep-learning
  - tensorflow
permalink: /:categories/:title/
---
`Sung Kim êµìˆ˜ë‹˜ ê°•ì˜ë¡œ ê³µë¶€í•˜ëŠ” ë¨¸ì‹ ëŸ¬ë‹ Section 2,3`

<!-- {% include adsense.html %} -->

### Linear regression
xì™€ yê°€ `linear relationship`ì„ ê°€ì§„ë‹¤ê³  íŒë‹¨ì´ ë“¤ ë•Œ, ì¼ì°¨ë°©ì •ì‹ í˜•íƒœì˜ ê°€ì„¤ì„ ì„¸ì›Œ ëª¨ë¸ë§í•˜ëŠ” ê²ƒì´ë‹¤.

<br/> 

#### Hypothesis
$H(x)=Wx+b$
ê°€ì„¤ì€ ì¼ì°¨ë°©ì •ì‹ì˜ í˜•íƒœë¥¼ ëˆë‹¤. WëŠ” weight, slope(ê¸°ìš¸ê¸°)ì´ê³  bëŠ” bias, intercept(ì ˆí¸)ì´ë‹¤.  
`Simplified hypothesis`: ì´ë¥¼ ê°„ì†Œí™”í•´ì„œ $H(x)=Wx$ë¡œ ë‚˜íƒ€ë‚´ê¸°ë„ í•œë‹¤.

<br/> 

### Residual(ì”ì°¨)
ì”ì°¨ëŠ” $H(X)-y$, ëª¨ë¸ì—ì„œ ì˜ˆì¸¡í•œ ê°’ - ì‹¤ì œ ì˜ˆì¸¡í•œ ê°’ì´ë‹¤.

<br/> 

### RSS(residual sum of squares)
ì”ì°¨ëŠ” ìŒìˆ˜ê°€ ë  ìˆ˜ë„, ì–‘ìˆ˜ê°€ ë  ìˆ˜ë„ ìˆê¸° ë•Œë¬¸ì— squareì²˜ë¦¬í•´ì„œ ëª¨ë“  ì”ì°¨ë¥¼ ë”í•œë‹¤. RSSë¥¼ ì‹ìœ¼ë¡œ ë‚˜íƒ€ë‚´ë©´ $\sum_{i=1}^m (H(X_i)-y_i)^2$ì´ë‹¤.

<br/> 

### Cost function
$cost(W,b) = 1/m * \sum_{i=1}^m (H(X_i)-y_i)^2$

ì¢‹ì€ ëª¨ë¸ì„ ìœ„í•´ì„œëŠ” ì”ì°¨ë¥¼ ìµœì†Œí™” í•´ì•¼í•œë‹¤. ì´ ì”ì°¨ í•©ì˜ í‰ê· ì„ êµ¬í•˜ëŠ” í•¨ìˆ˜ë¥¼ `Cost function`ì´ë¼ê³  ë¶€ë¥´ëŠ”ë°, **ì´ costë¥¼ minimize í•˜ëŠ” w,bë¥¼ ì°¾ì•„ì•¼í•œë‹¤.** ì´ ì‹ì€ ê²°êµ­ ëª¨ë¸ì˜ Total Errorë¥¼ ë³´ì—¬ì¤€ë‹¤ê³  í•  ìˆ˜ ìˆê² ë‹¤. ì´ ì‹ì€ `Mean Squared Error`ë¼ê³ ë„ ë¶ˆë¦°ë‹¤. 

`Simplified cost function`: biasë¥¼ ê³ ë ¤í•˜ì§€ ì•Šê³  $cost = 1/m * \sum_{i=n}^m (W(X^i)-y^i)^2$ë¡œ ë‚˜íƒ€ë‚´ê¸°ë„ í•œë‹¤. Cost functionì€ `Loss function`ì´ë¼ê³ ë„ ë¶ˆë¦°ë‹¤.

<br/> 

### Gradient descent algorithm(ê²½ì‚¬ í•˜ê°•ë²•)
Cost functionì„ minimizeí•˜ëŠ” w,bë¥¼ ì°¾ì•„ë‚´ëŠ” ì•Œê³ ë¦¬ì¦˜

![gda](/assets/images/gda.png){: width="60%" height="60%"}

`Gradient`ëŠ” `ê²½ì‚¬`ë¥¼, `Descent`ëŠ” `ë‚´ë ¤ê°„`ì„ ì˜ë¯¸í•œë‹¤. ê²½ì‚¬ë„ë¥¼ ë”°ë¼ì„œ ë‚´ë ¤ê°€ë‹¤ê°€ ê²½ì‚¬ë„ê°€ 0ì´ ë˜ì—ˆì„ ë•Œ stopí•˜ëŠ” ì•Œê³ ë¦¬ì¦˜ì´ë¼ê³  í•  ìˆ˜ ìˆë‹¤.

* 0,0ì—ì„œ ì‹œì‘ (ë˜ëŠ” ë‹¤ë¥¸ ê°’ì—ì„œ ì‹œì‘)
* W,b ì„ ì¡°ê¸ˆ ë°”ê¾¸ê³  ì¤„ì¼ ìˆ˜ ìˆëŠ”ì§€ í™•ì¸í•œë‹¤.
* íŒŒë¼ë¯¸í„° (W,b)ë¥¼ ë°”ê¿€ë•Œë§ˆë‹¤ cost(W,b)ë¥¼ ê°€ì¥ ì¤„ì¼ìˆ˜ ìˆëŠ” ê²½ì‚¬ë„ (gradient)ë¥¼ ì„ íƒí•œë‹¤.
* ë¯¸ë¶„ì„ ì´ìš©í•´ êµ¬í•˜ëŠ”ë°, tensorflowì—ì„œ ìì²´ì ìœ¼ë¡œ êµ¬í•´ì£¼ê¸° ë•Œë¬¸ì— ê±±ì • ì•ˆí•´ë„ ëœë‹¤.

W := W - a(cost(W)ë¥¼ ë¯¸ë¶„í•œ ê°’)

* descentê°€ +ì´ë©´, -ë¡œ ì´ë™, descentê°€ -ì´ë©´, +ë¡œ ì´ë™
* ê¸°ìš¸ê¸°ê°€ ìŒìˆ˜ì´ë©´ Wì˜ ë³€í™”ëŸ‰ì´ ì–‘ìˆ˜ê°€ ë˜ë¯€ë¡œ ì˜¤ë¥¸ìª½ìœ¼ë¡œ ì´ë™, ê¸°ìš¸ê¸°ê°€ ì–‘ìˆ˜ì´ë©´ Wì˜ ë³€í™”ëŸ‰ì´ ìŒìˆ˜ê°€ ë˜ë¯€ë¡œ ì™¼ìª½ìœ¼ë¡œ ì´ë™

ì—¬ê¸°ì„œ a(alpha)ëŠ” learning rateë¼ê³  ë¶ˆë¦¬ëŠ”, stepì„ ì¡°ì ˆí•˜ëŠ” ê°’ì´ë‹¤.

Learning rateë¥¼ ì ì ˆí•˜ê²Œ ì„¤ì •í•´ì•¼ í•˜ëŠ” ì´ìœ ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.

1. Learning rateê°€ ë„ˆë¬´ í¬ë©´, ë°œì‚°í•´ë²„ë¦¬ëŠ” ê²½ìš°ê°€ ë°œìƒí•œë‹¤. (Overshooting)
2. Learning rateê°€ ë„ˆë¬´ ì‘ìœ¼ë©´, ì¼ë‹¨ ìˆ˜ë ´í•˜ëŠ” ì†ë„ê°€ ë„ˆë¬´ ëŠë¦¬ê³ , local minimumì— ë¹ ì§ˆ í™•ë¥ ì´ ì¦ê°€í•œë‹¤.

**ì´ê²ƒì´ trainingì˜ ì›ë¦¬ì´ë‹¤.**
1. random weightsë¡œ trainingì„ ì‹œì‘í•˜ê³ .
2. ê¸°ìš¸ê¸°ì˜ ë°˜ëŒ€ ë°©í–¥ìœ¼ë¡œ weightsë¥¼ ìˆ˜ì •í•˜ë©° training ì‹œí‚¨ë‹¤.

<br/> 

### Delta Rule
The delta rule learning formula for weight adjustment.

$$âˆ†ğ‘¤ = c(t_i-f(net_j))f'(net_j)x_k$$

* $c$ = learning rate
* $t_i$ = desired output
* $f(x)$ = actual output values of the $i_\text{th}$ node
* $f'$ = The derivative
* $x_k$ = the $k_\text{th}$ input to node i

[ë¸íƒ€ ê·œì¹™ì— ëŒ€í•´ ì˜ ì„¤ëª…ëœ ë¸”ë¡œê·¸ ê¸€ì„ ì²¨ë¶€í•œë‹¤.](http://blog.naver.com/PostView.nhn?blogId=infoefficien&logNo=220906916685&parentCategoryNo=&categoryNo=529&viewDate=&isShowPopularPosts=true&from=search)

<br/> 

### Convex function
ì‹œì‘ì ì— ìƒê´€ì—†ì´ ë„ì°©ì ì´ ê°™ì€ function

Cost functionì€ convex functionì´ì—¬ì•¼ í•œë‹¤. í™•ì¸í•˜ê³  GDAë¥¼ ì‚¬ìš©í•´ì•¼í•œë‹¤! í•˜ì§€ë§Œ ë‹¤í–‰íˆ ìš°ë¦¬ hypothesisê°€ convex functionì´ê¸° ë•Œë¬¸ì— ëŠ˜ minimumì„ ì°¾ì„ ìˆ˜ ìˆë‹¤.
