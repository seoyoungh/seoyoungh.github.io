---
layout: post
title:  "Introduction to Deep Learning"
date: 2019-09-27
use_math: true
categories:
  - deep-learning-zero-to-all
tags:
  - deep-learning
  - tensorflow
permalink: /:categories/:title/
---
`Sung Kim 교수님 강의로 공부하는 머신러닝 Section 8`

<!-- {% include adsense.html %} -->

### Activation Functions

![activation_func](/assets/images/activation_func.png){: width="50%" height="50%"}

- 뉴런 구조를 본따서 만든 것
- 신호 $x$가 들어오고, 거기에 weight를 곱하고, 곱한 값들의 summation을 해준 뒤 bias를 더해준다.
- 그러면 `activation function`이란 것에서 value가 특정 값보다 크면 1을, 작으면 0을 output으로 갖는다.

1960년대 이전에는, 이런 logistic regression을 hardware로 구현했다. 이때 뉴런은 전선으로 연결하고, weight 학습은 다이얼로 조절했다.

<br/>

### XOR Problem
![xor](/assets/images/xor.png){: width="50%" height="50%"}

가장 왼쪽의 OR 문제와 AND 문제는 linearly separable하다. 기존 방식으로도 이를 해결할 수 있었다.

하지만 두 값이 같을 때, `0,0 & 1,1` 일 때는 0을 output으로 갖고, 두 값이 다를 때, `0,1 & 1,0`일 때는 1을 output으로 갖는 XOR 문제는 linearly separable하지 않아서 풀 수 없었다.

Minsky 교수는 레이어를 여러 개 갖는 퍼셉트론인 `Multilayer Perceptron (MLP)`를 제안했다. 하지만 각각의 weight, bias를 학습시킬 수 없다는 문제 역시 지적했다. 이 문제는 그렇게 풀 수 없는 듯 보였다.

하지만, `Backpropagation`, `Convolutional Neural Network`가 등장하며 이를 해결할 수 있게 되었다. CNN은 layer들을 조각내서 보낸 다음, 나중에 합치는 알고리즘이다. 알파고에도 이 알고리즘이 사용되었다.

하지만 기존의 `Backpropagation`은 레이어의 개수가 증가하면, 성능이 떨어졌다. 오히려 SVM이나 RandomForest가 더 simple하지만, 더 잘 작동했다.

2006년 Hinton 교수가, 2007년 Bengio 교수가 breakthrough가 되는 두 논문을 발표한다.

2006년에는 굉장히 깊은 신경망을 학습할 수 없는 이유가 weight의 초기값을 잘못 주었기 때문이라는 것을 밝혔다. 2007년에는 깊게 신경망을 구축하면 굉장히 복잡한 문제를 풀 수 있다고 보였다. 이후 다시 사람들이 Deep Learning을 연구하기 시작했다.

Neural Network는 labeling을 넘어 이제 explain까지 할 수 있다. 현재 딥러닝은 Image Recognition, Deep API, Speech, Game Automation, AlphaGo 등에 다양하게 쓰이고 있다.
