---
layout: post
title:  "Logistic Classification"
date: 2019-09-11
use_math: true
categories:
  - deep-learning-zero-to-all
tags:
  - deep-learning
  - tensorflow
permalink: /:categories/:title/
---
`Sung Kim 교수님 강의로 공부하는 머신러닝 Section 5`

`가볍게 시작하는 통계학습 with R (ISLR의 번역서)`도 참고하여 작성했습니다.

<!-- {% include adsense.html %} -->

### Classification
분류, 관측치에 대한 질적 반응변수를 예측하는 방법

반응변수 Y가 quantitive(양적)인 linear regression과 달리 classification은 반응변수 Y가 `qualitative(질적)`이다. 질적 변수는 흔히 `categorical(범주형)`이라고 불린다.

이 질적 반응변수를 예측하는 것이 `classification`이다. 관측치를 ‘분류한다’는 것은 관측치를 범주 또는 클래스로 할당한다는 것이다.

이때 사용되는 분류기법, classifier는 다양하다. logistic regression, linear discriminant analysis, K-nearest neighbors 가 대표적인 예이다. 컴퓨터를 더 많이 사용하는 방법에는 generalized addictive model, tree, random forest, boosting, support vector machine(SVM)이 존재한다.

<br/> 

### 분류의 예시
* Spam E-mail Detection: `Spam` or `Ham`
* Credit Card Fraudulent Transaction Detection: `Legitimate` or `Fraud`

Y를 위와 같이 분류하기 위해 상관관계가 있을 것이라 예상되는 X로 질적 변수를 예측한다. X, Y의 상관관계 예시를 들면 다음과 같다.

X: `Income(연간 소득)`, `Balance(월 카드 대금)`
Y: `Default(카드 연체 여부)`

연간 소득, 월 카드 대금과 신용카드 연체 여부간의 상관관계를 알아보는 분류이다.

<br/> 

### Linear Regression?
반응변수가 질적인 경우 선형회귀는 적절하지 않다. 그 이유를 알아보자.

<br/> 

#### Case 1: Multi level
선형회귀를 적용하기 위해 3개 이상의 질적 변수를 1, 2, 3 ... 과 같이 양적 변수로 변경할 수 있다. 하지만 이러한 인코딩은 어떻게 변경하든 `순서(Ordering)`이 있다. Ordering에 따라 상관관계가 전혀 달라지고, 다른 선형모델이 만들어지면 결국 관측치의 예측값이 달라질 것이다.

예로, `약한 / 적당한 / 심한`과 같은 질적 변수를 `1 / 2 / 3`으로 변환했다고 하자. 하지만 `약한 — 적당한` 사이의 차이가 `적당한 — 심한`의 차이보다 크다면, `1 / 4 / 5`와 같은 변환이 적절할 것이다.

하지만 이런 질적인 정도의 차이를 정확히 양적으로 변환할 수는 없다. 결국, **선형회귀를 위해 레벨이 3 이상인 질적 반응변수를 양적 반응변수로 변경하는 자연스런 방법은 없다.**

<br/> 

#### Case 2: Two-level (Binary)
이진 질적 변수의 경우, Ordering에 있어서는 multi-level보다는 덜 문제가 된다. 하지만, **선형회귀모델을 적용했을 때, 0과 1사이를 벗어나는 확률값이 나올 수 있다.**

조금 더 이해를 돕기 위해 Binary Classification에 선형회귀모델을 적용한 예를 들겠다.

```
학생이 해당 시험에서 Pass할지 or Fail할지를 판단하기 위해 공부시간을 X로 두자. Y는 Fail(0), Pass(1)으로 Encoding한다. 우리는 Y > 0.5이면 Pass로 예측할 수 있다.이를 우리가 구한 linear model에 적용했을 때 X > 20h이면 Pass라는 결과를 얻었다고 하자.
```

만약 한 학생의 공부시간(X)이 기존에 관측되었던 값들의 범위를 많이 벗어난 100h라면, 우리의 모델에서 예측한 이 학생이 Pass 할 *확률은 1을 넘어갈 수 있다.* 반대로 극단적으로 공부시간이 적어 0에 가깝다면, *음수의 확률이 나올 수도 있다.*

모든 확률값을 0과 1 사이에 두기 위해 수정한 방법이 `logistic regression`이다. 이 때 쓰이는 함수가 `logistic function (sigmoid function)`이다.

<br/> 

### Logistic Regression
* 로지스틱 회귀는 변수가 0 or 1인 binary classification에 쓰이는 회귀이다.
* Logistic function, Sigmoid Function을 사용하는 방법이다.
* 정확도가 높은 알고리즘으로 알려져 있다.
* Neural Network, Deep Learning에 적용되는 중요한 컴포넌트이다.

반응변수 Y를 직접 모델링하지 않고, **Y가 특정 범주에 속하는 확률을 모델링한다.**

아까의 연체 확률 예시를 다시 살펴보자. 주어진 balance에 대한 연체확률은 다음과 같이 나타낼 수 있다.

$$Pr(default=Yes|balance)$$

이는 줄여서 $p(balance)$로 나타낼 수 있고, 범위는 0과 1사이이다.

결국, 우리는 $p(X) = Pr(Y = 1\|X)$와 $X$ 사이의 관계를 모델링해야 한다.

<br/> 

#### Logistic Hypothesis

$$H(X) = \frac{1}{1+e^-W^TX}$$

로지스틱 회귀는 이러한 형태의 Hypothesis를 갖는다.

<br/> 

### Logistic function, Sigmoid Function
: 이 함수는 모든 X의 값에 대해 0과 1사이의 값을 제공한다.

$z = Wx + b$를 0 ~ 1 사이로 압축시켜주는 함수

$$g(z) = \frac{1}{1+e^-z}$$

함수는 항상 S-형태로, X값에 상관없이 합리적인 예측값을 얻을 수 있다.

이때의 확률은 training dataset 내 $Y=1$의 비율과도 동일하다.

<br/> 

### Cost function
`Sigmoid function`은 `Convex function`이 아니다! `Local minimum`, `Global minimum`이 다를 수 있다. 따라서, cost function을 새로 만들어준다. `exponentional`과 상반되는 `log`를 취해준다. 식은 다음과 같다.

$$cost(W) = \frac{1}{m}\sum \ C(H(x),y)$$

$C(H(x),y)$의 식은 $y=1$일 때와, $y=0$일 때로 나뉜다.

**i) y=1**

$$-log(H(x))$$

-> 로그함수의 모양에 따라 H(x)가 1이라면 cost는 0이 나온다. 반면, H(x)가 0이라면 cost는 무한으로 커진다.

**ii) y=0**

$$-log(1-H(x))$$

-> 로그함수의 모양에 따라 H(x)가 0이라면 cost는 0이 나온다. 반면, H(x)가 1이라면 cost는 무한으로 커진다.

정리해서 하나의 식으로 적으면 다음과 같다.

$$C(H(x),y)=-ylog(H(x))-(1-y)log(1-H(x))$$

cost를 최소화 하기 위해서는 linear regression과 동일하게 `Gradient Decent Algorithm`을 적용한다.
